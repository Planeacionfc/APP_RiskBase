{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones para estrucuturar los datos de la vista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrfc._cyrfc import Connection, ABAPApplicationError\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from datetime import datetime, timedelta\n",
    "import win32com.client as win32\n",
    "from pretty_html_table import build_table\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from typing import Dict, Any\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class SAPConnection:\n",
    "    def __init__(self, ashost, sysnr, client, user, passwd, lang):\n",
    "        \"\"\"\n",
    "        Inicializa la conexi√≥n a SAP.\n",
    "        \"\"\"\n",
    "        self.connection_params = {\n",
    "            'ashost': ashost,\n",
    "            'sysnr': sysnr,\n",
    "            'client': client,\n",
    "            'user': user,\n",
    "            'passwd': passwd,\n",
    "            'lang': lang\n",
    "        }\n",
    "        self.connection = None\n",
    "\n",
    "    def open_connection(self):\n",
    "        \"\"\"\n",
    "        Abre una conexi√≥n a SAP con los par√°metros especificados.\n",
    "        \"\"\"\n",
    "        if self.connection is None:\n",
    "            self.connection = Connection(**self.connection_params)\n",
    "        return self.connection\n",
    "\n",
    "    def close_connection(self):\n",
    "        \"\"\"\n",
    "        Cierra la conexi√≥n a SAP.\n",
    "        \"\"\"\n",
    "        if self.connection:\n",
    "            self.connection.close()\n",
    "            self.connection = None\n",
    "\n",
    "    def execute_query(self, query_name, view_id, parameters):\n",
    "        \"\"\"\n",
    "        Ejecuta una consulta a SAP con par√°metros din√°micos.\n",
    "\n",
    "        Args:\n",
    "            query_name (str): Nombre del query SAP.\n",
    "            view_id (str): ID de la vista.\n",
    "            parameters (list of tuples): Lista de par√°metros en formato (nombre, valor).\n",
    "\n",
    "        Returns:\n",
    "            dict: Resultado de la consulta.\n",
    "        \"\"\"\n",
    "        self.open_connection()\n",
    "        # Construir la lista de par√°metros din√°micamente\n",
    "        formatted_parameters = [{\"NAME\": p[0], \"VALUE\": p[1]} for p in parameters]\n",
    "        formatted_parameters += [\n",
    "        ]\n",
    "\n",
    "        print(\"üõ† Sending Parameters to SAP BW:\") ##DEBUGGING\n",
    "        for param in formatted_parameters:\n",
    "            print(f\"{param['NAME']} = {param['VALUE']}\")\n",
    "\n",
    "\n",
    "        try: \n",
    "            result = self.connection.call(\n",
    "                \"RRW3_GET_QUERY_VIEW_DATA\",\n",
    "                I_QUERY=query_name,\n",
    "                I_VIEW_ID=view_id,\n",
    "                I_T_PARAMETER=formatted_parameters\n",
    "            )\n",
    "            print(\"‚úÖ Query executed successfully!\") ##DEBUGGING\n",
    "            return result\n",
    "        except ABAPApplicationError as error:\n",
    "            print(\"Error en SAP: \" + error.message)\n",
    "            return None\n",
    "        finally:\n",
    "            self.close_connection()\n",
    "\n",
    "    def extract_axis_data(self, axis_data):\n",
    "        \"\"\" Extrae informaci√≥n de las columnas (metadatos) de los datos del eje.\n",
    "        \n",
    "        Args:\n",
    "            axis_data (list): Lista de diccionarios que representa los datos de los ejes de la respuesta de SAP.\n",
    "        \n",
    "        Returns:\n",
    "            pandas.DataFrame: DataFrame con la informaci√≥n de las columnas como CHANM y sus etiquetas CAPTION.\n",
    "        \"\"\"\n",
    "        column_info = []\n",
    "        # Iterar a trav√©s de todos los datos de los ejes\n",
    "        for data in axis_data:\n",
    "            for set_item in data['SET']:\n",
    "                # Agregar solo si el item es nuevo\n",
    "                if not any(d['CHANM'] == set_item['CHANM'] for d in column_info):\n",
    "                    column_info.append({\n",
    "                        'CHANM': set_item['CHANM'],\n",
    "                        'CAPTION': set_item['CAPTION']\n",
    "                    })\n",
    "    \n",
    "        return pd.DataFrame(column_info)\n",
    "\n",
    "    def extract_axis_info(self, axis_data):\n",
    "        \"\"\"\n",
    "        Extrae y muestra informaci√≥n detallada sobre cada eje para ayudar a comprender la estructura del cubo.\n",
    "\n",
    "        Args:\n",
    "            axis_data (list): Lista de diccionarios que contienen informaci√≥n sobre los ejes.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Un DataFrame que contiene detalles sobre cada eje y sus caracter√≠sticas.\n",
    "        \"\"\"\n",
    "        # Inicializaci√≥n de la lista para almacenar la informaci√≥n de los ejes\n",
    "        axis_info = []\n",
    "        # Iteraci√≥n a trav√©s de cada eje en los datos proporcionados\n",
    "        for axis in axis_data:\n",
    "            # Iteraci√≥n a trav√©s de cada caracter√≠stica del eje\n",
    "            for char in axis.get('CHARS', []):\n",
    "                axis_info.append({\n",
    "                    'AXIS': axis['AXIS'],\n",
    "                    'CHANM': char['CHANM'],\n",
    "                    'CAPTION': char['CAPTION'],\n",
    "                    'CHATYP': char['CHATYP'],\n",
    "                    'DETAILS': f\"Presentaciones: {char['CHAPRSNT']}, Atributos: {len(char.get('ATTRINM', []))}\"\n",
    "                })\n",
    "        # Creaci√≥n de un DataFrame con la informaci√≥n recopilada\n",
    "        return pd.DataFrame(axis_info)\n",
    "\n",
    "    def clean_data(self, axis_data, cell_data):\n",
    "        \"\"\"\n",
    "        Transforma los datos brutos de ejes y celdas del cubo SAP en un DataFrame estructurado.\n",
    "\n",
    "        Args:\n",
    "            axis_data (list): Lista que contiene detalles de los ejes (Columnas).\n",
    "            cell_data (list): Lista que contiene los valores de las celdas relacionados con los ejes.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Un DataFrame estructurado que combina tanto los datos de los ejes como de las celdas.\n",
    "        \"\"\"\n",
    "        # Extracci√≥n de detalles de los ejes\n",
    "        data = []\n",
    "        for entry in axis_data:\n",
    "            if entry['AXIS'] == '001':  # ejemplo, ajustar basado en el caso de uso real\n",
    "                for item in entry['SET']:\n",
    "                    data.append({\n",
    "                        'TUPLE_ORDINAL': item['TUPLE_ORDINAL'],\n",
    "                        'CHANM': item['CHANM'],\n",
    "                        'CAPTION': item['CAPTION'],\n",
    "                        'CHAVL': item['CHAVL'],\n",
    "                        'MONTH': item.get('CHAVL_EXT', '')  # campo de ejemplo\n",
    "                    })\n",
    "        \n",
    "        # Creaci√≥n de DataFrame a partir de los datos de los ejes\n",
    "        df_final_data = pd.DataFrame(data)\n",
    "        \n",
    "        # Creaci√≥n de DataFrame a partir de los datos de las celdas\n",
    "        #df_final_cells = pd.DataFrame(cell_data)\n",
    "        #df_final_cells = df_final_cells.rename(columns={'CELL_ORDINAL': 'TUPLE_ORDINAL'})\n",
    "        \n",
    "        # Fusi√≥n de los DataFrames en 'TUPLE_ORDINAL'\n",
    "        #merged_df_final = pd.merge(df_final_data, df_final_cells, on='TUPLE_ORDINAL', how='left')\n",
    "        \n",
    "        return df_final_data\n",
    "        #return merged_df_final #KEVIN\n",
    "\n",
    "    def data_structuring(self, df_final, axis_info=None, values=['CAPTION']):\n",
    "        \"\"\"\n",
    "        Organiza los datos en un formato estructurado, transformando filas repetidas en columnas.\n",
    "        \n",
    "        Args:\n",
    "            df_final (pd.DataFrame): DataFrame que contiene los datos a organizar.\n",
    "            axis_info (pd.DataFrame): DataFrame opcional que contiene informaci√≥n sobre los ejes para identificar las columnas din√°micamente.\n",
    "            values (list): Lista con los nombres de las columnas que desea obtener del df_final\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame organizado con filas repetidas transformadas en columnas.\n",
    "        \"\"\"\n",
    "        if axis_info is not None:\n",
    "            # Usamos la informaci√≥n del eje para identificar columnas CLAVE\n",
    "            key_columns = axis_info[axis_info['CHATYP'] == '1']['CHANM'].tolist()\n",
    "        \n",
    "        # Filtrar el DataFrame original para mantener solo las columnas CLAVE y sus valores\n",
    "        filtered_df_final = df_final[df_final['CHANM'].isin(key_columns)]\n",
    "        \n",
    "        # Usar pivot_table para manejar m√∫ltiples valores de 'values'\n",
    "        pivot_df_final = filtered_df_final.pivot_table(index='TUPLE_ORDINAL', columns='CHANM', \n",
    "                                        values=values, \n",
    "                                        aggfunc='last').reset_index()\n",
    "\n",
    "        # Aplanar las columnas Multindex resultantes\n",
    "        pivot_df_final.columns = [' '.join(col).strip() for col in pivot_df_final.columns.values]\n",
    "\n",
    "        # Construir diccionario de renombrado si axis_info est√° DISPONIBLE\n",
    "        if axis_info is not None:\n",
    "            rename_dict = {}\n",
    "            for _, row in axis_info.iterrows():\n",
    "                if row['CHANM'] in key_columns:\n",
    "                    for suffix in values:\n",
    "                        old_col_name = f\"{suffix} {row['CHANM']}\"\n",
    "                        new_col_name = f\"{row['CAPTION']}-{suffix}\"\n",
    "                        rename_dict[old_col_name] = new_col_name\n",
    "            pivot_df_final = pivot_df_final.rename(columns=rename_dict)\n",
    "\n",
    "        return pivot_df_final\n",
    "    \n",
    "    def extract_all_data(self, column_names: List[str],query_name, view_id, params) -> pd.DataFrame:\n",
    "\n",
    "        self.open_connection()\n",
    "        raw_data = self.execute_query(query_name, view_id, params)\n",
    "\n",
    "\n",
    "        # Obtenemos diccionario con los nombres originales de las columnas\n",
    "        axis_info = self.extract_axis_info(raw_data['E_AXIS_INFO'])\n",
    "        # Obtenemos informaci√≥n combinada y transformada\n",
    "        data_clean = self.clean_data(raw_data['E_AXIS_DATA'], raw_data['E_CELL_DATA'])\n",
    "        # Estructuramos columnas y filas para un mejor entendimiento y visualizaci√≥n\n",
    "        df_final_axis_values = self.data_structuring(data_clean, axis_info, ['CAPTION','CHAVL','VALUE']) #kevin\n",
    "        \n",
    "        for record in raw_data['E_CELL_DATA']:\n",
    "            print(record)\n",
    "        \n",
    "        # Extraer los datos de las celdas del cubo y organizarlos en un DataFrame\n",
    "        cell_records = [\n",
    "            {'CELL_ORDINAL': record['CELL_ORDINAL'], 'VALUE': record['VALUE']}\n",
    "            for record in raw_data['E_CELL_DATA']\n",
    "        ]\n",
    "        df_final_cell = pd.DataFrame(cell_records)\n",
    "\n",
    "        df_final_cell['Group'] = df_final_cell.index // (len(column_names))\n",
    "\n",
    "        # Generar el DataFrame con las columnas ordenadas de acuerdo a `column_names`\n",
    "        df_final_cell_values = pd.DataFrame({\n",
    "            name: df_final_cell.groupby('Group')['VALUE'].nth(i).values\n",
    "            for i, name in enumerate(column_names)\n",
    "        })\n",
    "        return df_final_cell_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consultar los datos de la vista en SAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_sap():\n",
    "    \"\"\"\n",
    "    Conecta a SAP y obtiene el stock del mes ANTERIOR al mes actual.\n",
    "    \"\"\"\n",
    "    today = datetime.now()\n",
    "    month = today.strftime(\"%m.%Y\")\n",
    "\n",
    "    sap_conn = SAPConnection(\n",
    "        ashost=os.getenv(\"ASHOST\"),\n",
    "        sysnr=os.getenv(\"SYSNR\"),\n",
    "        client=os.getenv(\"CLIENT\"),\n",
    "        user=os.getenv(\"USER_SAP\"),\n",
    "        passwd=os.getenv(\"PASSWORD_SAP\"),\n",
    "        lang=\"ES\",\n",
    "    )\n",
    "\n",
    "    params = [\n",
    "        (\"VAR_ID_6\", \"0I_CMNTH                      0004\"),\n",
    "        (\"VAR_VALUE_LOW_EXT_6\", month),\n",
    "        (\"VAR_VALUE_HIGH_EXT_6\", month),\n",
    "    ]\n",
    "\n",
    "    result = sap_conn.execute_query(\"ZICM_CM03_Q001\", \"Z_BASE_RIESGO\", params)\n",
    "\n",
    "    # Primero extraemos la informacion de los ejes y los limpiamos\n",
    "    axis_info = sap_conn.extract_axis_info(result[\"E_AXIS_INFO\"])\n",
    "\n",
    "    # Limpia los datos de la consulta y la informacion de las celdas\n",
    "    data_clean = sap_conn.clean_data(result[\"E_AXIS_DATA\"], result[\"E_CELL_DATA\"])\n",
    "\n",
    "    # Con la informacion de los ejes, estructuramos los datos para que se ajusten a la estructura de un DataFrame\n",
    "    df_final_axis_values = sap_conn.data_structuring(\n",
    "        data_clean, axis_info, [\"CAPTION\", \"CHAVL\"]\n",
    "    )\n",
    "\n",
    "    # Estos son los nombres de las columnas que se van a crear en el DataFrame final\n",
    "    column_names = ['Costo Unitario Real',\n",
    "                    'Inventario Disponibl', \n",
    "                    'Inventario No Dispon', \n",
    "                    'Valor OBSOLETO', \n",
    "                    'Valor BLOQUEADO MM', \n",
    "                    'Valor Total MM', \n",
    "                    'Permanencia'\n",
    "                ]\n",
    "\n",
    "    # Con la informacion de las celdas, creamos un diccionario que contiene los valores de cada celda\n",
    "    cell_records = [\n",
    "        {'CELL_ORDINAL': record['CELL_ORDINAL'], 'VALUE': record['VALUE']}\n",
    "        for record in result['E_CELL_DATA']\n",
    "    ]\n",
    "\n",
    "    # Con el diccionario, creamos un DataFrame que contiene los valores de las celdas\n",
    "    df_final_cell = pd.DataFrame(cell_records)\n",
    "\n",
    "    # Agregamos una columna 'Group' que indica a que grupo pertenece cada celda. Se hace con // (divisin entera)\n",
    "    df_final_cell['Group'] = df_final_cell.index // (len(column_names))\n",
    "\n",
    "    # Creamos un nuevo DataFrame que contiene los valores de las celdas agrupados por el grupo\n",
    "    df_final_cell_values = pd.DataFrame({\n",
    "        name: df_final_cell.groupby('Group')['VALUE'].nth(i).values\n",
    "        for i, name in enumerate(column_names)\n",
    "    })\n",
    "\n",
    "    # Finalmente, concatenamos los dos DataFrames en uno solo, con la informacion de los ejes y los valores de las celdas\n",
    "    df_final_combined = pd.concat([df_final_axis_values, df_final_cell_values], axis=1)\n",
    "\n",
    "    # Renombrar columnas\n",
    "    df_final_combined = df_final_combined.rename(\n",
    "        columns={\n",
    "            'Material-CAPTION': 'Descripci√≥n', \n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Eliminar columnas innecesarias\n",
    "    df_final_combined = df_final_combined.drop(\n",
    "        columns=[\n",
    "            \"TUPLE_ORDINAL\",\n",
    "            \"Lote-CAPTION\",\n",
    "            \"Fecha entrada-CAPTION\",\n",
    "            \"Centro-CAPTION\",\n",
    "            \"Unidad medida-CAPTION\",\n",
    "            \"Codigo Almacen Cliente-CAPTION\",\n",
    "            \"Rango Cobertura-CAPTION\",\n",
    "            \"Creado el-CAPTION\",\n",
    "            \"Fecha Bloqueado-CAPTION\",\n",
    "            \"Fecha Obsoleto-CAPTION\",\n",
    "            \"Fech. Fabricaci√≥n-CAPTION\",\n",
    "            \"Rango de Permanencia-CAPTION\",\n",
    "            \"Rango Bloqueado-CAPTION\",\n",
    "            \"Rango Vencidos-CAPTION\",\n",
    "            \"Rango Obsoleto-CAPTION\",\n",
    "            \"Rango Pr√≥ximos a Ven-CAPTION\",\n",
    "            \"Rango Pr√≥x.Vencer MM-CAPTION\",\n",
    "            \"Fech, Caducidad/Fech Pref. Consumo-CAPTION\",\n",
    "            \"A√±o natural/Mes-CHAVL\",\n",
    "            \"Indicador Stock Espec.-CHAVL\",\n",
    "            \"Marca de QM-CHAVL\",\n",
    "            \"Tipo Material Inventario-CHAVL\",\n",
    "            \"Negocio Inventarios-CHAVL\",\n",
    "            \"Tipo de Material (I)-CHAVL\",\n",
    "            \"N√∫m.stock.esp.-CHAVL\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Eliminar sufijos de los nombres de columnas\n",
    "    df_final_combined.columns = df_final_combined.columns.str.replace(\"-CHAVL\", \"\").str.replace(\n",
    "        \"-CAPTION\", \"\"\n",
    "    )\n",
    "\n",
    "    # Convertir todas las columnas a may√∫sculas\n",
    "    df_final_combined = df_final_combined.apply(\n",
    "        lambda x: x.str.upper() if x.dtype == \"object\" else x\n",
    "    )\n",
    "    df_final_combined.columns = df_final_combined.columns.str.upper()\n",
    "\n",
    "    # Convertir columnas num√©ricas\n",
    "    columnas_numericas = [\n",
    "        \"COSTO UNITARIO REAL\",\n",
    "        \"INVENTARIO DISPONIBL\",\n",
    "        \"INVENTARIO NO DISPON\",\n",
    "        \"VALOR OBSOLETO\",\n",
    "        \"VALOR BLOQUEADO MM\",\n",
    "        \"VALOR TOTAL MM\",\n",
    "        \"PERMANENCIA\",\n",
    "    ]\n",
    "\n",
    "    for col in columnas_numericas:\n",
    "        if col in df_final_combined.columns:\n",
    "            df_final_combined[col] = pd.to_numeric(df_final_combined[col], errors=\"coerce\")\n",
    "\n",
    "    # Convertir columnas de fechas\n",
    "    columnas_fecha = [\n",
    "        \"FECHA ENTRADA\",\n",
    "        \"CREADO EL\",\n",
    "        \"FECHA BLOQUEADO\",\n",
    "        \"FECHA OBSOLETO\",\n",
    "        \"FECH. FABRICACI√ìN\",\n",
    "        \"FECH, CADUCIDAD/FECH PREF. CONSUMO\",\n",
    "    ]\n",
    "\n",
    "    for col in columnas_fecha:\n",
    "        if col in df_final_combined.columns:\n",
    "            df_final_combined[col] = pd.to_datetime(\n",
    "                df_final_combined[col], errors=\"coerce\"\n",
    "            ).dt.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "    df_final_combined = pd.DataFrame(df_final_combined)\n",
    "\n",
    "    return df_final_combined\n",
    "\n",
    "def upload_dataframe_to_db(df_final_combined: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Sube el DataFrame 'df_final_combined' a la base de datos en la tabla 'InventarioBaseRiesgo'.\n",
    "    Se utiliza SQLAlchemy para establecer la conexi√≥n y el m√©todo to_sql de pandas para insertar\n",
    "    todos los registros (append). Se asume que la tabla ya existe y que los nombres de columnas en \n",
    "    el DataFrame coinciden exactamente con los de la tabla en la base de datos.\n",
    "    \n",
    "    Args:\n",
    "        df_final_combined (pd.DataFrame): DataFrame con las columnas y el orden requeridos.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    connection_string = (\n",
    "            \"mssql+pyodbc://{user}:{pwd}@{server}/{db}\"\n",
    "            \"?driver=ODBC+Driver+17+for+SQL+Server\"\n",
    "        ).format(\n",
    "            user=os.getenv(\"DB_USER\"),\n",
    "            pwd=os.getenv(\"DB_PASSWORD\"),\n",
    "            server=os.getenv(\"DB_SERVER\"),\n",
    "            db=os.getenv(\"DATABASE\")\n",
    "        )\n",
    "    \n",
    "    # Crear el engine de SQLAlchemy con fast_executemany habilitado\n",
    "    engine = create_engine(connection_string, fast_executemany=True)\n",
    "    \n",
    "    try:\n",
    "        # Insertar datos en la tabla InventarioBaseRiesgo. \n",
    "        # if_exists='append' se utiliza para agregar los datos sin reemplazar la tabla.\n",
    "        df_final_combined.to_sql(\n",
    "            name='InventarioBaseRiesgo',\n",
    "            con=engine,\n",
    "            if_exists='append',\n",
    "            index=False,\n",
    "            chunksize=1000  # Tama√±o del chunk para inserciones masivas\n",
    "        )\n",
    "        print(\"Datos subidos correctamente a InventarioBaseRiesgo.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error al subir el DataFrame a la base de datos:\", e)\n",
    "    finally:\n",
    "        engine.dispose()\n",
    "\n",
    "def export_dataframe_to_excel(df: pd.DataFrame, filename: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Exporta un DataFrame a un archivo Excel con la fecha actual y lo guarda en una ruta espec√≠fica.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame a exportar\n",
    "        filename: Nombre base del archivo (opcional)\n",
    "\n",
    "    Returns:\n",
    "        str: Ruta del archivo Excel creado\n",
    "    \"\"\"\n",
    "    # Definir la ruta de destino\n",
    "    output_dir = r\"C:\\Users\\prac.planeacionfi\\OneDrive - Prebel S.A BIC\\Escritorio\\PRUEBAS BASE RIESGO\"\n",
    "\n",
    "    # Crear el nombre del archivo con la fecha actual\n",
    "    current_date = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "    if not filename:\n",
    "        filename = f\"An√°lisis_BaseRiesgo_Final_{current_date}.xlsx\"\n",
    "\n",
    "    # Crear la ruta completa\n",
    "    file_path = os.path.join(output_dir, filename)\n",
    "\n",
    "    # Asegurar que el directorio existe\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Exportar a Excel\n",
    "    try:\n",
    "        df.to_excel(file_path, index=False, sheet_name=\"Base de Riesgo\")\n",
    "        print(f\"Archivo Excel creado exitosamente: {file_path}\")\n",
    "        return file_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error al exportar a Excel: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_avon_natura(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filtra un DataFrame para quedarse solo con los materiales\n",
    "    cuya 'MARCA DE QM' sea 'AVON' o 'NATURA'.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame original que contiene la columna 'MARCA DE QM'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Nuevo DataFrame con solo las filas de AVON y NATURA,\n",
    "                      reindexado de 0 a N-1.\n",
    "    \"\"\"\n",
    "    mask = df[\"MARCA DE QM\"].isin([\"AVON\", \"NATURA\"])\n",
    "    return df.loc[mask].reset_index(drop=True)\n",
    "\n",
    "def filter_marca_otros(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Devuelve solo las filas donde 'MARCA DE QM' NO sea ni 'AVON' ni 'NATURA'.\n",
    "    Esto permite trabajar con el resto de las marcas por separado.\n",
    "    \"\"\"\n",
    "    # Aseguramos uniformidad en may√∫sculas\n",
    "    df = df.copy()\n",
    "    df[\"MARCA DE QM\"] = df[\"MARCA DE QM\"].str.upper()\n",
    "    \n",
    "    # Filtramos inversamente\n",
    "    mask = ~df[\"MARCA DE QM\"].isin([\"AVON\", \"NATURA\"])\n",
    "    return df[mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ† Sending Parameters to SAP BW:\n",
      "VAR_ID_6 = 0I_CMNTH                      0004\n",
      "VAR_VALUE_LOW_EXT_6 = 05.2025\n",
      "VAR_VALUE_HIGH_EXT_6 = 05.2025\n",
      "‚úÖ Query executed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prac.planeacionfi\\AppData\\Local\\Temp\\ipykernel_6420\\600308896.py:143: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_final_combined[col] = pd.to_datetime(\n",
      "C:\\Users\\prac.planeacionfi\\AppData\\Local\\Temp\\ipykernel_6420\\600308896.py:143: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_final_combined[col] = pd.to_datetime(\n"
     ]
    }
   ],
   "source": [
    "# 1. Obtienes el DataFrame completo\n",
    "df_final_combined = get_data_sap()\n",
    "\n",
    "# 2. Filtro para obtener solo los datos de AVON y NATURA\n",
    "df_avon_natura = filter_avon_natura(df_final_combined)\n",
    "\n",
    "# 3. Filtro para obtener los datos de otras marcas\n",
    "df_otras_marcas = filter_marca_otros(df_final_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sql_engine():\n",
    "    \"\"\"\n",
    "    Establece la conexi√≥n a SQL Server mediante SQLAlchemy.\n",
    "    Configura los datos de conexi√≥n utilizando variables de entorno.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        connection_string = (\n",
    "            \"mssql+pyodbc://{user}:{pwd}@{server}/{db}\"\n",
    "            \"?driver=ODBC+Driver+17+for+SQL+Server\"\n",
    "        ).format(\n",
    "            user=os.getenv(\"DB_USER\"),\n",
    "            pwd=os.getenv(\"DB_PASSWORD\"),\n",
    "            server=os.getenv(\"DB_SERVER\"),\n",
    "            db=os.getenv(\"DATABASE\")\n",
    "        )\n",
    "        engine = create_engine(connection_string)\n",
    "        print(\"Conexi√≥n exitosa a la base de datos.\")\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        print(f\"Error al conectar a la base de datos: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def execute_query(query):\n",
    "    \"\"\"\n",
    "    Ejecuta el query en la base de datos y retorna un DataFrame utilizando SQLAlchemy.\n",
    "    \"\"\"\n",
    "    engine = get_sql_engine()\n",
    "    try:\n",
    "        df = pd.read_sql(query, engine)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al ejecutar el query: {str(e)}\")\n",
    "        df = pd.DataFrame()  # Retorna un DataFrame vac√≠o en caso de error\n",
    "    finally:\n",
    "        engine.dispose()  # Cierra la conexi√≥n\n",
    "    return df\n",
    "\n",
    "def get_inventario_matriz():\n",
    "    \"\"\"\n",
    "    Extrae todos los registros de la tabla InventarioMatriz.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        id_politica_base_riesgo,\n",
    "        subsegmento,\n",
    "        negocio,\n",
    "        estado,\n",
    "        cobertura\n",
    "    FROM InventarioMatriz\n",
    "    \"\"\"\n",
    "    return execute_query(query)\n",
    "\n",
    "def get_matrices_base_riesgo():\n",
    "    \"\"\"\n",
    "    Extrae todos los registros de la tabla MatrizBaseRiesgo.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT\n",
    "        id_politica_base_riesgo,\n",
    "        concatenado,\n",
    "        segmento,\n",
    "        permanencia,\n",
    "        factor_prov,\n",
    "        clasificacion,\n",
    "        tipo_matriz\n",
    "    FROM MatrizBaseRiesgo\n",
    "    \"\"\"\n",
    "    return execute_query(query)\n",
    "\n",
    "def df_matrices_merge():\n",
    "    \"\"\"\n",
    "    Extrae los datos de ambas tablas, los unifica utilizando pd.merge() y convierte\n",
    "    todos los campos de texto a may√∫sculas de forma vectorizada.\n",
    "    \"\"\"\n",
    "    df_inventario = get_inventario_matriz()\n",
    "    df_matrices = get_matrices_base_riesgo()\n",
    "    \n",
    "    # Unificaci√≥n utilizando la columna en com√∫n 'id_politica_base_riesgo'\n",
    "    df_matrices_merge = pd.merge(df_inventario, df_matrices, \n",
    "                        on=\"id_politica_base_riesgo\", \n",
    "                        how=\"inner\")  # Cambia 'inner' por 'left' o 'outer' seg√∫n lo requieras\n",
    "    \n",
    "    # Convertir los campos de tipo string a may√∫sculas de forma vectorizada\n",
    "    for col in df_matrices_merge.select_dtypes(include=[\"object\"]).columns:\n",
    "        df_matrices_merge[col] = df_matrices_merge[col].str.upper()\n",
    "    \n",
    "    # Convertir 'factor_prov' a float y normalizarlo\n",
    "    df_matrices_merge['factor_prov'] = df_matrices_merge['factor_prov'].apply(lambda x: x/100)\n",
    "    \n",
    "    return df_matrices_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_matrices_avon_natura():\n",
    "    \"\"\"\n",
    "    Extrae los datos de ambas tablas, las unifica utilizando pd.merge(),\n",
    "    convierte todos los campos de texto a may√∫sculas de forma vectorizada,\n",
    "    normaliza el factor provisional y, finalmente, filtra solo las filas\n",
    "    donde 'tipo_matriz' == 'MATRIZ NATURACO'.\n",
    "    \"\"\"\n",
    "    # 1. Extraer los DataFrames base\n",
    "    df_inventario = get_inventario_matriz()\n",
    "    df_matrices   = get_matrices_base_riesgo()\n",
    "    \n",
    "    # 2. Unirlos por la clave for√°nea\n",
    "    df = pd.merge(\n",
    "        df_inventario,\n",
    "        df_matrices,\n",
    "        on=\"id_politica_base_riesgo\",\n",
    "        how=\"inner\"  # o 'left' / 'outer' seg√∫n necesidad\n",
    "    )\n",
    "    \n",
    "    # 3. Pasar a may√∫sculas todas las columnas de texto\n",
    "    text_cols = df.select_dtypes(include=\"object\").columns\n",
    "    for c in text_cols:\n",
    "        df[c] = df[c].str.upper()\n",
    "    \n",
    "    # 4. Normalizar el factor provisional (de porcentaje a [0‚Äì1])\n",
    "    df[\"factor_prov\"] = df[\"factor_prov\"].astype(float) / 100.0\n",
    "    \n",
    "    # 5. Filtrar solo las filas de 'Matriz NaturaCo'\n",
    "    #    (ten en cuenta que ya convertimos todo a may√∫sculas)\n",
    "    df = df[df[\"tipo_matriz\"] == \"MATRIZ NATURACO\"].reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def df_matrices_otros_tipos():\n",
    "    \"\"\"\n",
    "    Devuelve solo las filas de df_merge donde 'tipo_matriz' sea distinto de 'Matriz NaturaCo'.\n",
    "    √ötil para aislar todas las dem√°s matrices.\n",
    "    \"\"\"\n",
    "    # 1. Extraer los DataFrames base\n",
    "    df_inventario = get_inventario_matriz()\n",
    "    df_matrices   = get_matrices_base_riesgo()\n",
    "    \n",
    "    # 2. Unirlos por la clave for√°nea\n",
    "    df = pd.merge(\n",
    "        df_inventario,\n",
    "        df_matrices,\n",
    "        on=\"id_politica_base_riesgo\",\n",
    "        how=\"inner\"  # o 'left' / 'outer' seg√∫n necesidad\n",
    "    )\n",
    "    \n",
    "    # 3. Pasar a may√∫sculas todas las columnas de texto\n",
    "    text_cols = df.select_dtypes(include=\"object\").columns\n",
    "    for c in text_cols:\n",
    "        df[c] = df[c].str.upper()\n",
    "    \n",
    "    # 4. Normalizar el factor provisional (de porcentaje a [0‚Äì1])\n",
    "    df[\"factor_prov\"] = df[\"factor_prov\"].astype(float) / 100.0\n",
    "    \n",
    "    # Filtramos aquellas filas cuyo tipo de matriz NO sea 'MATRIZ NATURACO'\n",
    "    mask = df[\"tipo_matriz\"] != \"MATRIZ NATURACO\"\n",
    "    \n",
    "    return df[mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexi√≥n exitosa a la base de datos.\n",
      "Conexi√≥n exitosa a la base de datos.\n",
      "Conexi√≥n exitosa a la base de datos.\n",
      "Conexi√≥n exitosa a la base de datos.\n",
      "Conexi√≥n exitosa a la base de datos.\n",
      "Conexi√≥n exitosa a la base de datos.\n"
     ]
    }
   ],
   "source": [
    "df_matrices_merge = df_matrices_merge()\n",
    "df_matrices_avon_natura = df_matrices_avon_natura()\n",
    "df_matrices_otros_tipos = df_matrices_otros_tipos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L√≥gica para AVON y NATURA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_marks() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Retorna un diccionario con el mapeo de marcas QM a marcas concatenadas.\n",
    "    \"\"\"\n",
    "    # TODO: Implementar el diccionario de marcas seg√∫n la l√≥gica de negocio\n",
    "    return {\n",
    "        \"ACCESORIOS\": \"ACCESORIOS\",\n",
    "        \"ADIDAS\": \"ADIDAS\",\n",
    "        \"AGATHA RUIZ DE LA PRADA\": \"AGATHA RUIZ DE LA PRADA\",\n",
    "        \"ALICORP\": \"ALICORP\",\n",
    "        \"AMAZON\": \"AMAZON\",\n",
    "        \"AMWAY\": \"AMWAY\",\n",
    "        \"ARDEN FOR MEN\": \"AFM/CFM\",\n",
    "        \"AVON\": \"AVON\",\n",
    "        \"BALANCE\": \"BALANCE\",\n",
    "        \"BANCO PREBEL\": \"BANCO PREBEL\",\n",
    "        \"BEAUTYHOLICS\": \"UTOPICK\",\n",
    "        \"BIO OIL\": \"BIO OIL\",\n",
    "        \"BIOTECNIK\": \"BIOTECNIK\",\n",
    "        \"BURTS_BEES\": \"BURT'S BEES\",\n",
    "        \"CADIVEU\": \"CADIVEU\",\n",
    "        \"calculateA\": \"calculateA\",\n",
    "        \"CATRICE\": \"CATRICE\",\n",
    "        \"CONNECT FOR MEN\": \"AFM/CFM\",\n",
    "        \"COSMETRIX\": \"COSMETRIX\",\n",
    "        \"COVER GIRL\": \"COVER GIRL\",\n",
    "        \"DIAL\": \"DIAL\",\n",
    "        \"DOVE\": \"DOVE\",\n",
    "        \"DYCLASS\": \"DYCLASS\",\n",
    "        \"ECAR\": \"ECAR\",\n",
    "        \"EL EXITO\": \"EL EXITO\",\n",
    "        \"ELIZABETH ARDEN\": \"ELIZABETH ARDEN\",\n",
    "        \"ESSENCE\": \"ESSENCE\",\n",
    "        \"FAMILIA\": \"FAMILIA\",\n",
    "        \"FEBREZE\": \"FEBREZE\",\n",
    "        \"FISA\": \"FISA\",\n",
    "        \"HASK\": \"HASK\",\n",
    "        \"HENKEL\": \"HENKEL\",\n",
    "        \"HERBAL ESSENCES\": \"HERBAL ESSENCES\",\n",
    "        \"IMPORTADOS PROCTER\": \"IMPORTADOS PROCTER\",\n",
    "        \"JERONIMO MARTINS\": \"JERONIMO MARTINS\",\n",
    "        \"KANABECARE\": \"KANABECARE\",\n",
    "        \"KIMBERLY\": \"KIMBERLY\",\n",
    "        \"KOBA\": \"D1\",\n",
    "        \"L&G ASOCIADOS\": \"L&G ASOCIADOS\",\n",
    "        \"LA POPULAR\": \"LA POPULAR\",\n",
    "        \"LEONISA\": \"LEONISA\",\n",
    "        \"LOCATEL\": \"LOCATEL\",\n",
    "        \"LOREAL\": \"LOREAL\",\n",
    "        \"LOVE, BEAUTY AND PLANET\": \"LOVE, BEAUTY AND PLANET\",\n",
    "        \"MAUI\": \"MAUI\",\n",
    "        \"MAX FACTOR\": \"MAX FACTOR\",\n",
    "        \"MAX FACTOR EXPORTACI√ìN\": \"MAX FACTOR\",\n",
    "        \"MAX FACTOR GLOBAL\": \"MAX FACTOR\",\n",
    "        \"MF COL + EXP\": \"MAX FACTOR\",\n",
    "        \"MF GLOBAL\": \"MAX FACTOR\",\n",
    "        \"MILAGROS\": \"MILAGROS\",\n",
    "        \"MONCLER\": \"MONCLER\",\n",
    "        \"MORROCCANOIL\": \"MORROCCANOIL\",\n",
    "        \"NATURA\": \"NATURA\",\n",
    "        \"NATURAL PARADISE\": \"NATURAL PARADISE\",\n",
    "        \"NIVEA\": \"NIVEA\",\n",
    "        \"NOPIKEX\": \"NOPIKEX\",\n",
    "        \"NOVAVENTA FPT\": \"NOVAVENTA FPT\",\n",
    "        \"NUDE\": \"NUDE\",\n",
    "        \"OGX\": \"OGX\",\n",
    "        \"OLAY\": \"OLAY\",\n",
    "        \"OMNILIFE\": \"OMNILIFE\",\n",
    "        \"OTRAS\": \"OTRAS\",\n",
    "        \"PREBEL\": \"PREBEL\",\n",
    "        \"QVS\": \"ACCESORIOS\",\n",
    "        \"SALLY HANSEN\": \"SALLY HANSEN\",\n",
    "        \"SIN ASIGNAR\": \"SIN ASIGNAR\",\n",
    "        \"SOLLA\": \"SOLLA\",\n",
    "        \"ST. IVES\": \"ST. IVES\",\n",
    "        \"UBU\": \"ACCESORIOS\",\n",
    "        \"UNILEVER\": \"UNILEVER\",\n",
    "        \"VENTA DIRECTA COSM√âTICOS\": \"VENTA DIRECTA COSM√âTICOS\",\n",
    "        \"VIT√ö\": \"VIT√ö\",\n",
    "        \"VIT√ö  EXPORTACI√ìN\": \"VIT√ö\",\n",
    "        \"WELLA CONSUMO\": \"WELLA CONSUMO\",\n",
    "        \"WELLA PROFESSIONAL\": \"WELLA PROFESSIONAL\",\n",
    "        \"YARDLEY\": \"YARDLEY\",\n",
    "        \"CAT√ÅLOGO DE PRODUCTOS\": \"CAT√ÅLOGO DE PRODUCTOS\",\n",
    "        \"D1\": \"D1\",\n",
    "        \"WORMSER\": \"WORMSER\",\n",
    "        \"PROCTER AND GAMBLE\": \"P&G\",\n",
    "        \"DAVINES\": \"DAVINES\",\n",
    "        \"LA FABRIL\": \"LA FABRIL\",\n",
    "        \"REVOX\": \"REVOX\",\n",
    "        \"TENDENCIAS AB\": \"TENDENCIAS AB\",\n",
    "    }\n",
    "\n",
    "\n",
    "def insert_subsegmentacion() -> Dict[str, str]:\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    RETORNA UN DICCIONARIO CON EL MAPEO DE MARCAS QM A SUBSEGMENTACI√ìN.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"ACCESORIOS\": \"OTROS\",\n",
    "        \"ADIDAS\": \"OTROS\",\n",
    "        \"AGATHA RUIZ DE LA PRADA\": \"OTROS\",\n",
    "        \"ALICORP\": \"FULL\",\n",
    "        \"AMAZON\": \"RETAILERS\",\n",
    "        \"AMWAY\": \"SISTEMA DE VENTAS\",\n",
    "        \"ARDEN FOR MEN\": \"OTROS\",\n",
    "        \"AVON\": \"FULL\",\n",
    "        \"BALANCE\": \"FULL\",\n",
    "        \"BANCO PREBEL\": \"FULL\",\n",
    "        \"BEAUTYHOLICS\": \"OTROS\",\n",
    "        \"BIO OIL\": \"OTROS\",\n",
    "        \"BIOTECNIK\": \"FULL\",\n",
    "        \"BURTS_BEES\": \"OTROS\",\n",
    "        \"CADIVEU\": \"PROFESIONALES\",\n",
    "        \"CALA\": \"FULL\",\n",
    "        \"CATRICE\": \"OTROS\",\n",
    "        \"CONNECT FOR MEN\": \"OTROS\",\n",
    "        \"COSMETRIX\": \"OTROS\",\n",
    "        \"COVER GIRL\": \"OTROS\",\n",
    "        \"DIAL\": \"RETAILERS\",\n",
    "        \"DOVE\": \"OTROS\",\n",
    "        \"DYCLASS\": \"SISTEMA DE VENTAS\",\n",
    "        \"ECAR\": \"FULL\",\n",
    "        \"EL EXITO\": \"RETAILERS\",\n",
    "        \"ELIZABETH ARDEN\": \"OTROS\",\n",
    "        \"ESSENCE\": \"OTROS\",\n",
    "        \"FAMILIA\": \"FULL\",\n",
    "        \"FEBREZE\": \"OTROS\",\n",
    "        \"FISA\": \"FULL\",\n",
    "        \"HASK\": \"OTROS\",\n",
    "        \"HENKEL\": \"FULL\",\n",
    "        \"HERBAL ESSENCES\": \"OTROS\",\n",
    "        \"IMPORTADOS PROCTER\": \"OTROS\",\n",
    "        \"JERONIMO MARTINS\": \"RETAILERS\",\n",
    "        \"KANABECARE\": \"OTROS\",\n",
    "        \"KIMBERLY\": \"FULL\",\n",
    "        \"KOBA\": \"RETAILERS\",\n",
    "        \"L&G ASOCIADOS\": \"FULL\",\n",
    "        \"LA POPULAR\": \"RETAILERS\",\n",
    "        \"LEONISA\": \"SISTEMA DE VENTAS\",\n",
    "        \"LOCATEL\": \"RETAILERS\",\n",
    "        \"LOREAL\": \"FULL\",\n",
    "        \"LOVE, BEAUTY AND PLANET\": \"OTROS\",\n",
    "        \"MAUI\": \"OTROS\",\n",
    "        \"MAX FACTOR\": \"OTROS\",\n",
    "        \"MAX FACTOR EXPORTACI√ìN\": \"OTROS\",\n",
    "        \"MAX FACTOR GLOBAL\": \"OTROS\",\n",
    "        \"MILAGROS\": \"SISTEMA DE VENTAS\",\n",
    "        \"MONCLER\": \"RETAILERS\",\n",
    "        \"MORROCCANOIL\": \"PROFESIONALES\",\n",
    "        \"NATURA\": \"FULL\",\n",
    "        \"NATURAL PARADISE\": \"OTROS\",\n",
    "        \"NIVEA\": \"FULL\",\n",
    "        \"NOPIKEX\": \"OTROS\",\n",
    "        \"NOVAVENTA FPT\": \"SISTEMA DE VENTAS\",\n",
    "        \"NUDE\": \"NUDE\",\n",
    "        \"OGX\": \"OTROS\",\n",
    "        \"OLAY\": \"OTROS\",\n",
    "        \"OMNILIFE\": \"SISTEMA DE VENTAS\",\n",
    "        \"OTRAS\": \"OTROS\",\n",
    "        \"PREBEL\": \"OTROS\",\n",
    "        \"QVS\": \"OTROS\",\n",
    "        \"SALLY HANSEN\": \"OTROS\",\n",
    "        \"SIN ASIGNAR\": \"FULL\",\n",
    "        \"SOLLA\": \"FULL\",\n",
    "        \"ST. IVES\": \"OTROS\",\n",
    "        \"UBU\": \"OTROS\",\n",
    "        \"UNILEVER\": \"TOLL\",\n",
    "        \"VENTA DIRECTA COSM√âTICOS\": \"FULL\",\n",
    "        \"VIT√ö\": \"OTROS\",\n",
    "        \"VIT√ö  EXPORTACI√ìN\": \"OTROS\",\n",
    "        \"WELLA CONSUMO\": \"OTROS\",\n",
    "        \"WELLA PROFESSIONAL\": \"PROFESIONALES\",\n",
    "        \"YARDLEY\": \"OTROS\",\n",
    "        \"D1\": \"RETAILERS\",\n",
    "        \"CAT√ÅLOGO DE PRODUCTOS\": \"RETAILERS\",\n",
    "        \"WORMSER\": \"RETAILERS\",\n",
    "        \"PROCTER AND GAMBLE\": \"FULL\",\n",
    "        \"DAVINES\": \"OTROS\",\n",
    "        \"LA FABRIL\": \"OTROS\",\n",
    "        \"REVOX\": \"OTROS\",\n",
    "        \"TENDENCIAS AB\": \"RETAILERS\"\n",
    "    }\n",
    "\n",
    "\n",
    "def insert_segments() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Retorna un diccionario con el mapeo de marcas QM a segmentaciones.\n",
    "    \"\"\"\n",
    "    # TODO: Implementar el diccionario de segmentaciones seg√∫n la l√≥gica de negocio\n",
    "    return {\n",
    "        \"OTROS CLIENTES DO\": \"DUE√ëOS DE CANAL\",\n",
    "        \"MARKETING PERSONAL\": \"DUE√ëOS DE CANAL\",\n",
    "        \"OMNILIFE\": \"DUE√ëOS DE CANAL\",\n",
    "        \"JERONIMO MARTINS\": \"DUE√ëOS DE CANAL\",\n",
    "        \"LEONISA\": \"DUE√ëOS DE CANAL\",\n",
    "        \"LOCATEL\": \"DUE√ëOS DE CANAL\",\n",
    "        \"NOVAVENTA\": \"DUE√ëOS DE CANAL\",\n",
    "        \"EL √âXITO\": \"DUE√ëOS DE CANAL\",\n",
    "        \"MILAGROS ENTERPRISE\": \"DUE√ëOS DE CANAL\",\n",
    "        \"LA POPULAR\": \"DUE√ëOS DE CANAL\",\n",
    "        \"D1\": \"DUE√ëOS DE CANAL\",\n",
    "        \"USA\": \"DUE√ëOS DE CANAL\",\n",
    "        \"USA\": \"DUE√ëOS DE CANAL\",\n",
    "        \"EL EXITO\": \"DUE√ëOS DE CANAL\",\n",
    "        \"NOVAVENTA FPT\": \"DUE√ëOS DE CANAL\",\n",
    "        \"MILAGROS\": \"DUE√ëOS DE CANAL\",\n",
    "        \"WORMSER\": \"DUE√ëOS DE CANAL\",\n",
    "        \"TENDENCIAS AB\": \"DUE√ëOS DE CANAL\",\n",
    "        \"LA FABRIL\": \"DUE√ëOS DE CANAL\",\n",
    "        \"UNILEVER\": \"EXPERTOS LOCALES\",\n",
    "        \"NATURA\": \"EXPERTOS LOCALES\",\n",
    "        \"BIOTECNIK\": \"EXPERTOS LOCALES\",\n",
    "        \"NIVEA\": \"EXPERTOS LOCALES\",\n",
    "        \"BRITO\": \"EXPERTOS LOCALES\",\n",
    "        \"AVON\": \"EXPERTOS LOCALES\",\n",
    "        \"OTROS EXPERTOS LOCALES\": \"EXPERTOS LOCALES\",\n",
    "        \"ALICORP\": \"EXPERTOS LOCALES\",\n",
    "        \"SOLLA\": \"EXPERTOS LOCALES\",\n",
    "        \"ECAR\": \"EXPERTOS LOCALES\",\n",
    "        \"FISA\": \"EXPERTOS LOCALES\",\n",
    "        \"KIMBERLY\": \"EXPERTOS LOCALES\",\n",
    "        \"BELCORP\": \"EXPERTOS LOCALES\",\n",
    "        \"AMWAY\": \"EXPERTOS LOCALES\",\n",
    "        \"PROCTER AND GAMBLE\": \"EXPERTOS LOCALES\",\n",
    "        \"HENKEL\": \"EXPERTOS LOCALES\",\n",
    "        \"DIAL\": \"EXPERTOS LOCALES\",\n",
    "        \"BEIERSDORF\": \"EXPERTOS LOCALES\",\n",
    "        \"OTROS EXPERTOS LOCALES\": \"EXPERTOS LOCALES\",\n",
    "        \"FAMILIA\": \"EXPERTOS LOCALES\",\n",
    "        \"BALANCE\": \"EXPERTOS LOCALES\",\n",
    "        \"MAX FACTOR\": \"EXPERTOS NO LOCALES\",\n",
    "        \"DYCLASS\": \"EXPERTOS NO LOCALES\",\n",
    "        \"WELLA CONSUMO\": \"EXPERTOS NO LOCALES\",\n",
    "        \"BIO OIL\": \"EXPERTOS NO LOCALES\",\n",
    "        \"OGX\": \"EXPERTOS NO LOCALES\",\n",
    "        \"COVER GIRL\": \"EXPERTOS NO LOCALES\",\n",
    "        \"WELLA PROFESSIONAL\": \"EXPERTOS NO LOCALES\",\n",
    "        \"ADIDAS\": \"EXPERTOS NO LOCALES\",\n",
    "        \"ACCESORIOS\": \"EXPERTOS NO LOCALES\",\n",
    "        \"BURTS_BEES\": \"EXPERTOS NO LOCALES\",\n",
    "        \"NOPIKEX\": \"EXPERTOS NO LOCALES\",\n",
    "        \"QVS\": \"EXPERTOS NO LOCALES\",\n",
    "        \"UBU\": \"EXPERTOS NO LOCALES\",\n",
    "        \"ESSENCE\": \"EXPERTOS NO LOCALES\",\n",
    "        \"MORROCCANOIL\": \"EXPERTOS NO LOCALES\",\n",
    "        \"HASK\": \"EXPERTOS NO LOCALES\",\n",
    "        \"HERBAL ESSENCES\": \"EXPERTOS NO LOCALES\",\n",
    "        \"LOVE, BEAUTY AND PLANET\": \"EXPERTOS NO LOCALES\",\n",
    "        \"CATRICE\": \"EXPERTOS NO LOCALES\",\n",
    "        \"NATURAL PARADISE\": \"EXPERTOS NO LOCALES\",\n",
    "        \"OLAY\": \"EXPERTOS NO LOCALES\",\n",
    "        \"MID\": \"EXPERTOS NO LOCALES\",\n",
    "        \"SECRET\": \"EXPERTOS NO LOCALES\",\n",
    "        \"FEBREZE\": \"EXPERTOS NO LOCALES\",\n",
    "        \"TAMPAX\": \"EXPERTOS NO LOCALES\",\n",
    "        \"OFCORSS C.I HERMECO\": \"EXPERTOS NO LOCALES\",\n",
    "        \"CADIVEU\": \"EXPERTOS NO LOCALES\",\n",
    "        \"MAX FACTOR GLOBAL\": \"EXPERTOS NO LOCALES\",\n",
    "        \"SEBASTIAN\": \"EXPERTOS NO LOCALES\",\n",
    "        \"AFFRESH\": \"EXPERTOS NO LOCALES\",\n",
    "        \"COSMETRIX\": \"EXPERTOS NO LOCALES\",\n",
    "        \"INCENTIVOS MAX FACTOR\": \"EXPERTOS NO LOCALES\",\n",
    "        \"OTROS EXPERTOS NO LOCALES\": \"EXPERTOS NO LOCALES\",\n",
    "        \"DAVINES\": \"EXPERTOS NO LOCALES\",\n",
    "        \"P&G\": \"EXPERTOS NO LOCALES\",\n",
    "        \"REVOX\": \"EXPERTOS NO LOCALES\",\n",
    "        \"UTOPICK\": \"EXPERTOS NO LOCALES\",\n",
    "        \"IMPORTADOS PROCTER\": \"EXPERTOS NO LOCALES\",\n",
    "        \"ST. IVES\": \"EXPERTOS NO LOCALES\",\n",
    "        \"ARDEN FOR MEN\": \"MARCAS PROPIAS\",\n",
    "        \"NUDE\": \"MARCAS PROPIAS\",\n",
    "        \"ELIZABETH ARDEN\": \"MARCAS PROPIAS\",\n",
    "        \"YARDLEY\": \"MARCAS PROPIAS\",\n",
    "        \"VIT√ö\": \"MARCAS PROPIAS\",\n",
    "        \"PREBEL\": \"MARCAS PROPIAS\",\n",
    "        \"OTRAS MP\": \"MARCAS PROPIAS\",\n",
    "        \"AFM/CFM\": \"MARCAS PROPIAS\",\n",
    "        \"BODY CLEAR\": \"NO APLICA\",\n",
    "        \"OTRAS\": \"NO APLICA\",\n",
    "        \"GILLETTE\": \"NO APLICA\",\n",
    "        \"L&G ASOCIADOS\": \"NO APLICA\",\n",
    "        \"CAT√ÅLOGO DE PRODUCTOS\": \"NO APLICA\",\n",
    "        \"HINODE\": \"NO APLICA\",\n",
    "        \"PFIZER\": \"NO APLICA\",\n",
    "        \"CONTEXPORT DISNEY\": \"NO APLICA\",\n",
    "        \"SYSTEM PROFESSIONAL\": \"NO APLICA\",\n",
    "        \"WELONDA\": \"NO APLICA\",\n",
    "        \"SIN ASIGNAR\": \"NO APLICA\",\n",
    "        \"CAT√ÅLOGO DE PRODUCTOS\": \"NO APLICA\",\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_rango_permanencia(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Calcula el rango de permanencia basado en las condiciones especificadas.\n",
    "\n",
    "    Args:\n",
    "        row: Fila del DataFrame con las columnas necesarias\n",
    "\n",
    "    Returns:\n",
    "        str: Rango de permanencia calculado\n",
    "    \"\"\"\n",
    "    lote = row.get(\"LOTE\", None)\n",
    "    permanencia = row.get(\"PERMANENCIA\", None)\n",
    "    rango_permanencia = row.get(\"RANGO DE PERMANENCIA\", None)\n",
    "\n",
    "    if lote == \"222222\":\n",
    "        return \"1.MENOR DE 90 DIAS\"\n",
    "    elif permanencia == 0 and rango_permanencia == \"5.MAYOR O IGUAL A 360 DIAS\":\n",
    "        return \"5.ENTRE 360 Y 540 DIAS\"\n",
    "    elif rango_permanencia == \"5.MAYOR O IGUAL A 360 DIAS\":\n",
    "        if permanencia < 540:\n",
    "            return \"5.ENTRE 360 Y 540 DIAS\"\n",
    "        elif permanencia < 720:\n",
    "            return \"6.ENTRE 540 Y 720 DIAS\"\n",
    "        else:\n",
    "            return \"7.MAYOR DE 720 DIAS\"\n",
    "    else:\n",
    "        return rango_permanencia\n",
    "\n",
    "\n",
    "def calculate_status_cons(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Calcula el estado de consumo basado en las condiciones de vencimiento, bloqueo y obsolescencia.\n",
    "\n",
    "    Args:\n",
    "        row: Fila del DataFrame con las columnas necesarias\n",
    "\n",
    "    Returns:\n",
    "        str: Estado calculado (VENCIDO, BLOQUEADO, OBSOLETO, PAV o DISPONIBLE)\n",
    "    \"\"\"\n",
    "    rango_prox_vencer = row.get(\"RANGO PR√ìX.VENCER MM\")\n",
    "    valor_BLOQUEADO = row.get(\"VALOR BLOQUEADO MM\")\n",
    "    valor_OBSOLETO = row.get(\"VALOR OBSOLETO\")\n",
    "\n",
    "    if rango_prox_vencer == \"VENCIDO\":\n",
    "        return \"VENCIDO\"\n",
    "    elif valor_BLOQUEADO != 0:\n",
    "        return \"BLOQUEADO\"\n",
    "    elif valor_OBSOLETO != 0:\n",
    "        return \"OBSOLETO\"\n",
    "    elif rango_prox_vencer in [\"1.PAV 3 MESES\", \"2.PAV 4 A 6 MESES\"]:\n",
    "        return \"PAV\"\n",
    "    else:\n",
    "        return \"DISPONIBLE\"\n",
    "\n",
    "\n",
    "def calculate_valor_def(row: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Calcula el valor definitivo basado en el estado de consumo.\n",
    "\n",
    "    Args:\n",
    "        row: Fila del DataFrame con las columnas necesarias\n",
    "\n",
    "    Returns:\n",
    "        float: Valor definitivo calculado\n",
    "    \"\"\"\n",
    "    status_cons = row.get(\"STATUS CONS\")\n",
    "    valor_BLOQUEADO = row.get(\"VALOR BLOQUEADO MM\")\n",
    "    valor_total = row.get(\"VALOR TOTAL MM\")\n",
    "\n",
    "    return valor_BLOQUEADO if status_cons == \"BLOQUEADO\" else valor_total\n",
    "\n",
    "\n",
    "def calculate_rango_obsolescencia(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Calcula el rango de obsolescencia basado en las fechas de entrada y obsolescencia.\n",
    "\n",
    "    Args:\n",
    "        row: Fila del DataFrame con las columnas necesarias\n",
    "\n",
    "    Returns:\n",
    "        str: Rango de obsolescencia calculado\n",
    "    \"\"\"\n",
    "    status_cons = row.get(\"STATUS CONS\")\n",
    "    fecha_entrada = row.get(\"FECHA ENTRADA\")\n",
    "    fecha_OBSOLETO = row.get(\"FECHA OBSOLETO\")\n",
    "\n",
    "    if status_cons != \"OBSOLETO\" or pd.isna(fecha_entrada) or pd.isna(fecha_OBSOLETO):\n",
    "        return \"FALSO\"\n",
    "\n",
    "    dias_obsolescencia = (fecha_entrada - fecha_OBSOLETO).days\n",
    "\n",
    "    if dias_obsolescencia <= 90:\n",
    "        return \"1.MENOR DE 90 DIAS\"\n",
    "    elif 90 < dias_obsolescencia <= 180:\n",
    "        return \"2.ENTRE 90 Y 180 DIAS\"\n",
    "    elif 180 < dias_obsolescencia <= 270:\n",
    "        return \"3.ENTRE 180 Y 270 DIAS\"\n",
    "    elif 270 < dias_obsolescencia <= 360:\n",
    "        return \"4.ENTRE 270 Y 360 DIAS\"\n",
    "    elif 360 < dias_obsolescencia <= 540:\n",
    "        return \"5.ENTRE 360 Y 540 DIAS\"\n",
    "    elif 540 < dias_obsolescencia <= 720:\n",
    "        return \"6.ENTRE 540 Y 720 DIAS\"\n",
    "    else:\n",
    "        return \"7.MAYOR DE 720 DIAS\"\n",
    "\n",
    "\n",
    "def calculate_rango_vencido(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Calcula el rango de vencimiento basado en las fechas de entrada y caducidad.\n",
    "\n",
    "    Args:\n",
    "        row: Fila del DataFrame con las columnas necesarias\n",
    "\n",
    "    Returns:\n",
    "        str: Rango de vencimiento calculado\n",
    "    \"\"\"\n",
    "    status_cons = row.get(\"STATUS CONS\")\n",
    "    fecha_entrada = row.get(\"FECHA ENTRADA\")\n",
    "    fecha_caducidad = row.get(\"FECH, CADUCIDAD/FECH PREF. CONSUMO\")\n",
    "\n",
    "    if status_cons != \"VENCIDO\" or pd.isna(fecha_entrada) or pd.isna(fecha_caducidad):\n",
    "        return \"FALSO\"\n",
    "\n",
    "    dias_vencido = (fecha_entrada - fecha_caducidad).days\n",
    "\n",
    "    if dias_vencido <= 90:\n",
    "        return \"1.MENOR DE 90 DIAS\"\n",
    "    elif 90 < dias_vencido <= 180:\n",
    "        return \"2.ENTRE 90 Y 180 DIAS\"\n",
    "    elif 180 < dias_vencido <= 270:\n",
    "        return \"3.ENTRE 180 Y 270 DIAS\"\n",
    "    elif 270 < dias_vencido <= 360:\n",
    "        return \"4.ENTRE 270 Y 360 DIAS\"\n",
    "    elif 360 < dias_vencido <= 540:\n",
    "        return \"5.ENTRE 360 Y 540 DIAS\"\n",
    "    elif 540 < dias_vencido <= 720:\n",
    "        return \"6.ENTRE 540 Y 720 DIAS\"\n",
    "    else:\n",
    "        return \"7.MAYOR DE 720 DIAS\"\n",
    "\n",
    "\n",
    "def calculate_rango_bloqueado(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Calcula el rango de bloqueo basado en las fechas de entrada y bloqueo.\n",
    "\n",
    "    Args:\n",
    "        row: Fila del DataFrame con las columnas necesarias\n",
    "\n",
    "    Returns:\n",
    "        str: Rango de bloqueo calculado\n",
    "    \"\"\"\n",
    "    status_cons = row.get(\"STATUS CONS\")\n",
    "    fecha_entrada = row.get(\"FECHA ENTRADA\")\n",
    "    fecha_bloqueado = row.get(\"FECHA BLOQUEADO\")\n",
    "\n",
    "    if status_cons != \"BLOQUEADO\" or pd.isna(fecha_entrada) or pd.isna(fecha_bloqueado):\n",
    "        return \"FALSO\"\n",
    "\n",
    "    dias_vencido = (fecha_entrada - fecha_bloqueado).days\n",
    "\n",
    "    if dias_vencido <= 90:\n",
    "        return \"1.MENOR DE 90 DIAS\"\n",
    "    elif 90 < dias_vencido <= 180:\n",
    "        return \"2.ENTRE 90 Y 180 DIAS\"\n",
    "    elif 180 < dias_vencido <= 270:\n",
    "        return \"3.ENTRE 180 Y 270 DIAS\"\n",
    "    elif 270 < dias_vencido <= 360:\n",
    "        return \"4.ENTRE 270 Y 360 DIAS\"\n",
    "    elif 360 < dias_vencido <= 540:\n",
    "        return \"5.ENTRE 360 Y 540 DIAS\"\n",
    "    elif 540 < dias_vencido <= 720:\n",
    "        return \"6.ENTRE 540 Y 720 DIAS\"\n",
    "    else:\n",
    "        return \"7.MAYOR A 720 DIAS\"\n",
    "\n",
    "\n",
    "def calculate_tiempo_bloqueo(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Calcula el tiempo de bloqueo basado en las fechas de entrada y bloqueo.\n",
    "\n",
    "    Args:\n",
    "        row: Fila del DataFrame con las columnas necesarias\n",
    "\n",
    "    Returns:\n",
    "        str: Tiempo de bloqueo calculado\n",
    "    \"\"\"\n",
    "    fecha_entrada = row.get(\"FECHA ENTRADA\")\n",
    "    fecha_bloqueado = row.get(\"FECHA BLOQUEADO\")\n",
    "    \n",
    "    if pd.isna(fecha_entrada) or pd.isna(fecha_bloqueado):\n",
    "        return 0\n",
    "\n",
    "    tiempo_bloqueado = (fecha_entrada - fecha_bloqueado).days\n",
    "    \n",
    "    return tiempo_bloqueado    \n",
    "\n",
    "\n",
    "def calculate_rango_cons(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Calcula el rango de consumo final basado en el status y los rangos correspondientes.\n",
    "\n",
    "    Args:\n",
    "        row: Fila del DataFrame con las columnas necesarias\n",
    "\n",
    "    Returns:\n",
    "        str: Rango de consumo calculado\n",
    "    \"\"\"\n",
    "    status = row.get(\"STATUS CONS\")\n",
    "\n",
    "    if status == \"OBSOLETO\":\n",
    "        return row.get(\"RANGO OBSOLESCENCIA\")\n",
    "    elif status == \"VENCIDO\":\n",
    "        return row.get(\"RANGO VENCIDO 2\")\n",
    "    elif status == \"BLOQUEADO\":\n",
    "        return row.get(\"RANGO BLOQUEADO 2\")\n",
    "    elif status == \"PAV\":\n",
    "        return row.get(\"RANGO DE PERMANENCIA 2\")\n",
    "    else:\n",
    "        return row.get(\"RANGO DE PERMANENCIA 2\")\n",
    "\n",
    "\n",
    "def calculate_avon_natura_factor_and_class(row, lookup_dict):\n",
    "    \"\"\"\n",
    "    Calcula factor provisional y clasificaci√≥n solo para materiales de AVON y NATURA,\n",
    "    siguiendo la l√≥gica de la f√≥rmula de Excel proporcionada.\n",
    "    \"\"\"\n",
    "    seg        = row[\"SEGMENTACION\"]\n",
    "    status     = row[\"STATUS CONS\"]\n",
    "    tiempo     = row[\"TIEMPO BLOQUEO\"]\n",
    "    perm       = row[\"PERMANENCIA\"]\n",
    "    tipo_mat   = row[\"TIPO DE MATERIAL (I)\"]\n",
    "    negocio    = row[\"NEGOCIO INVENTARIOS\"]\n",
    "    rango_cons = str(row[\"RANGO CONS\"]).strip()\n",
    "    indic      = row[\"INDICADOR STOCK ESPEC.\"]\n",
    "\n",
    "    # 1) Segmento interno bloqueado poco tiempo\n",
    "    if seg in [\"MARCAS PROPIAS\", \"EXPERTOS NO LOCALES\", \"DUE√ëOS DE DEMANDA\"] \\\n",
    "       and status == \"BLOQUEADO\" and tiempo <= 30:\n",
    "        return 0.0, \"BAJO\"\n",
    "\n",
    "    # 2) Disponible o PAV y permanencia <= 30 en granel\n",
    "    if status in [\"DISPONIBLE\", \"PAV\"] \\\n",
    "       and perm <= 30 \\\n",
    "       and tipo_mat in [\"GRANEL FAB A TERCERO\", \"GRANEL\"]:\n",
    "        return 0.0, \"BAJO\"\n",
    "\n",
    "    # 3) Negocio FPT ‚Üí lookup con A2&AQ2&AV2\n",
    "    if negocio == \"FPT\":\n",
    "        key = f\"{negocio}{status}{rango_cons}\"\n",
    "        return lookup_dict.get(key, (0.0, \"BAJO\"))\n",
    "\n",
    "    # 4) Stock ‚â† W y obsoleto/bloqueado/vencido ‚Üí mismo lookup\n",
    "    if indic != \"W\" and status in [\"OBSOLETO\", \"BLOQUEADO\", \"VENCIDO\"]:\n",
    "        key = f\"{negocio}{status}{rango_cons}\"\n",
    "        return lookup_dict.get(key, (0.0, \"BAJO\"))\n",
    "\n",
    "    # 5) Stock ‚â† W y disponible ‚Üí lookup con cobertura+permanencia2\n",
    "    if indic != \"W\" and status == \"DISPONIBLE\":\n",
    "        key = (\n",
    "            str(row[\"RANGO COBERTURA\"]).strip()\n",
    "            + str(row[\"RANGO DE PERMANENCIA 2\"]).strip()\n",
    "        )\n",
    "        return lookup_dict.get(key, (0.0, \"BAJO\"))\n",
    "\n",
    "    # 6) Marca Avon/Natura y estado vencido/obsoleto/PAV ‚Üí primer d√≠gito\n",
    "    marca_qm = row[\"MARCA DE QM\"]\n",
    "    if marca_qm in [\"AVON\", \"NATURA\"] and status in [\"VENCIDO\", \"OBSOLETO\", \"PAV\"]:\n",
    "        try:\n",
    "            first_digit = int(rango_cons[0])\n",
    "            return (1.0, \"MUY ALTO\") if first_digit > 4 else (0.2, \"MEDIO\")\n",
    "        except:\n",
    "            return 0.0, \"BAJO\"\n",
    "\n",
    "    # Default\n",
    "    return 0.0, \"BAJO\"\n",
    "\n",
    "\n",
    "def calculate_base_riesgo_column(row: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Calcula el valor de la columna de base riesgo\n",
    "    \"\"\"\n",
    "    if row[\"CLAS BASE RIESGO\"] == \"BAJO\":\n",
    "        return 0.0\n",
    "    else:\n",
    "        return row[\"VALOR DEF\"]\n",
    "\n",
    "\n",
    "def calculate_provision_column(row: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Calcula el valor de la columna de provisi√≥n\n",
    "    \"\"\"\n",
    "    if row[\"MARCA DE QM\"] == \"OTRAS\":\n",
    "        return 0.0\n",
    "    else:\n",
    "        return row[\"VALOR DEF\"] * row['FACTOR PROV']\n",
    "\n",
    "\n",
    "def process_dataframe_avon_natura(df_avon_natura: pd.DataFrame,df_matrices_avon_natura: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Procesa el DataFrame aplicando todas las reglas de negocio en el orden espec√≠fico requerido.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame con los datos de SAP\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame procesado con todas las columnas calculadas\n",
    "    \"\"\"\n",
    "    # 1. A√±adir columnas formuladas de 'MARCA CONCAT' y 'SEGMENTACION'\n",
    "    df_avon_natura[\"MARCA CONCAT\"] = df_avon_natura[\"MARCA DE QM\"].apply(lambda x: insert_marks().get(x, \"\"))\n",
    "    df_avon_natura[\"SEGMENTACION\"] = df_avon_natura[\"MARCA DE QM\"].apply(\n",
    "        lambda x: insert_segments().get(x, \"OTRAS\")\n",
    "    )\n",
    "    df_avon_natura[\"SUBSEGMENTACION\"] = df_avon_natura[\"MARCA DE QM\"].apply(\n",
    "        lambda x: insert_subsegmentacion().get(x, \"\")\n",
    "    )\n",
    "\n",
    "    # 2. Calcular 'RANGO DE PERMANENCIA 2'\n",
    "    required_columns = {\"LOTE\", \"PERMANENCIA\", \"RANGO DE PERMANENCIA\"}\n",
    "    if required_columns.issubset(df_avon_natura.columns):\n",
    "        df_avon_natura[\"RANGO DE PERMANENCIA 2\"] = df_avon_natura.apply(calculate_rango_permanencia, axis=1)\n",
    "    else:\n",
    "        print(f\"Faltan columnas: {required_columns - set(df_avon_natura.columns)}\")\n",
    "\n",
    "    # 3. Calcular 'STATUS CONS'\n",
    "    required_columns = {\"RANGO PR√ìX.VENCER MM\", \"VALOR BLOQUEADO MM\", \"VALOR OBSOLETO\"}\n",
    "    if required_columns.issubset(df_avon_natura.columns):\n",
    "        df_avon_natura[\"STATUS CONS\"] = df_avon_natura.apply(calculate_status_cons, axis=1)\n",
    "    else:\n",
    "        print(f\"Faltan columnas: {required_columns - set(df_avon_natura.columns)}\")\n",
    "\n",
    "    # 4. Calcular 'VALOR DEF'\n",
    "    required_columns = {\"STATUS CONS\", \"VALOR BLOQUEADO MM\", \"VALOR TOTAL MM\"}\n",
    "    if required_columns.issubset(df_avon_natura.columns):\n",
    "        df_avon_natura[\"VALOR DEF\"] = df_avon_natura.apply(calculate_valor_def, axis=1)\n",
    "    else:\n",
    "        print(f\"Faltan columnas: {required_columns - set(df_avon_natura.columns)}\")\n",
    "\n",
    "    # 5. Reemplazar valores inv√°lidos\n",
    "    df_avon_natura.replace(\"#\", np.nan, inplace=True)\n",
    "\n",
    "    # 6. Convertir columnas de fecha\n",
    "    date_columns = [\n",
    "        \"FECHA ENTRADA\",\n",
    "        \"FECHA OBSOLETO\",\n",
    "        \"FECHA BLOQUEADO\",\n",
    "        \"FECH. FABRICACI√ìN\",\n",
    "        \"CREADO EL\",\n",
    "        \"FECH, CADUCIDAD/FECH PREF. CONSUMO\",\n",
    "    ]\n",
    "\n",
    "    for col in date_columns:\n",
    "        if col in df_avon_natura.columns:\n",
    "            df_avon_natura[col] = pd.to_datetime(df_avon_natura[col], format=\"%d/%m/%Y\", errors=\"coerce\")\n",
    "\n",
    "    # 7. Calcular 'RANGO OBSOLESCENCIA'\n",
    "    df_avon_natura[\"RANGO OBSOLESCENCIA\"] = df_avon_natura.apply(calculate_rango_obsolescencia, axis=1)\n",
    "\n",
    "    # 8. Calcular 'RANGO VENCIDO 2'\n",
    "    df_avon_natura[\"RANGO VENCIDO 2\"] = df_avon_natura.apply(calculate_rango_vencido, axis=1)\n",
    "\n",
    "    # 9. Calcular 'RANGO BLOQUEADO 2'\n",
    "    df_avon_natura[\"RANGO BLOQUEADO 2\"] = df_avon_natura.apply(calculate_rango_bloqueado, axis=1)\n",
    "\n",
    "    # 10. Calcular 'RANGO CONS'\n",
    "    df_avon_natura[\"RANGO CONS\"] = df_avon_natura.apply(calculate_rango_cons, axis=1)\n",
    "    \n",
    "    # 11. Calcular 'TIEMPO BLOQUEO'\n",
    "    df_avon_natura[\"TIEMPO BLOQUEO\"] = df_avon_natura.apply(calculate_tiempo_bloqueo, axis=1)\n",
    "\n",
    "    # Construir lookup_dict **una vez** antes del apply\n",
    "    lookup_dict = {\n",
    "        str(r[\"concatenado\"]).strip(): (r[\"factor_prov\"], r[\"clasificacion\"])\n",
    "        for _, r in df_matrices_avon_natura.iterrows()\n",
    "    }\n",
    "\n",
    "    # Aplicar fila a fila y asignar dos nuevas columnas\n",
    "    df_avon_natura[[\"FACTOR PROV\", \"CLAS BASE RIESGO\"]] = df_avon_natura.apply(\n",
    "        lambda row: pd.Series(calculate_avon_natura_factor_and_class(row, lookup_dict)),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # 13. BASE RIESGO\n",
    "    df_avon_natura[\"BASE RIESGO\"] = df_avon_natura.apply(\n",
    "        calculate_base_riesgo_column, axis=1\n",
    "    )\n",
    "    # 14. PROVISION\n",
    "    df_avon_natura[\"PROVISION\"] = df_avon_natura.apply(\n",
    "        calculate_provision_column, axis=1\n",
    "    )\n",
    "\n",
    "    return df_avon_natura\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_avon_natura = process_dataframe_avon_natura(df_avon_natura, df_matrices_avon_natura)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L√≥gica para el resto de marcas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_marks() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Retorna un diccionario con el mapeo de marcas QM a marcas concatenadas.\n",
    "    \"\"\"\n",
    "    # TODO: Implementar el diccionario de marcas seg√∫n la l√≥gica de negocio\n",
    "    return {\n",
    "        \"ACCESORIOS\": \"ACCESORIOS\",\n",
    "        \"ADIDAS\": \"ADIDAS\",\n",
    "        \"AGATHA RUIZ DE LA PRADA\": \"AGATHA RUIZ DE LA PRADA\",\n",
    "        \"ALICORP\": \"ALICORP\",\n",
    "        \"AMAZON\": \"AMAZON\",\n",
    "        \"AMWAY\": \"AMWAY\",\n",
    "        \"ARDEN FOR MEN\": \"AFM/CFM\",\n",
    "        \"AVON\": \"AVON\",\n",
    "        \"BALANCE\": \"BALANCE\",\n",
    "        \"BANCO PREBEL\": \"BANCO PREBEL\",\n",
    "        \"BEAUTYHOLICS\": \"UTOPICK\",\n",
    "        \"BIO OIL\": \"BIO OIL\",\n",
    "        \"BIOTECNIK\": \"BIOTECNIK\",\n",
    "        \"BURTS_BEES\": \"BURT'S BEES\",\n",
    "        \"CADIVEU\": \"CADIVEU\",\n",
    "        \"calculateA\": \"calculateA\",\n",
    "        \"CATRICE\": \"CATRICE\",\n",
    "        \"CONNECT FOR MEN\": \"AFM/CFM\",\n",
    "        \"COSMETRIX\": \"COSMETRIX\",\n",
    "        \"COVER GIRL\": \"COVER GIRL\",\n",
    "        \"DIAL\": \"DIAL\",\n",
    "        \"DOVE\": \"DOVE\",\n",
    "        \"DYCLASS\": \"DYCLASS\",\n",
    "        \"ECAR\": \"ECAR\",\n",
    "        \"EL EXITO\": \"EL EXITO\",\n",
    "        \"ELIZABETH ARDEN\": \"ELIZABETH ARDEN\",\n",
    "        \"ESSENCE\": \"ESSENCE\",\n",
    "        \"FAMILIA\": \"FAMILIA\",\n",
    "        \"FEBREZE\": \"FEBREZE\",\n",
    "        \"FISA\": \"FISA\",\n",
    "        \"HASK\": \"HASK\",\n",
    "        \"HENKEL\": \"HENKEL\",\n",
    "        \"HERBAL ESSENCES\": \"HERBAL ESSENCES\",\n",
    "        \"IMPORTADOS PROCTER\": \"IMPORTADOS PROCTER\",\n",
    "        \"JERONIMO MARTINS\": \"JERONIMO MARTINS\",\n",
    "        \"KANABECARE\": \"KANABECARE\",\n",
    "        \"KIMBERLY\": \"KIMBERLY\",\n",
    "        \"KOBA\": \"D1\",\n",
    "        \"L&G ASOCIADOS\": \"L&G ASOCIADOS\",\n",
    "        \"LA POPULAR\": \"LA POPULAR\",\n",
    "        \"LEONISA\": \"LEONISA\",\n",
    "        \"LOCATEL\": \"LOCATEL\",\n",
    "        \"LOREAL\": \"LOREAL\",\n",
    "        \"LOVE, BEAUTY AND PLANET\": \"LOVE, BEAUTY AND PLANET\",\n",
    "        \"MAUI\": \"MAUI\",\n",
    "        \"MAX FACTOR\": \"MAX FACTOR\",\n",
    "        \"MAX FACTOR EXPORTACI√ìN\": \"MAX FACTOR\",\n",
    "        \"MAX FACTOR GLOBAL\": \"MAX FACTOR\",\n",
    "        \"MF COL + EXP\": \"MAX FACTOR\",\n",
    "        \"MF GLOBAL\": \"MAX FACTOR\",\n",
    "        \"MILAGROS\": \"MILAGROS\",\n",
    "        \"MONCLER\": \"MONCLER\",\n",
    "        \"MORROCCANOIL\": \"MORROCCANOIL\",\n",
    "        \"NATURA\": \"NATURA\",\n",
    "        \"NATURAL PARADISE\": \"NATURAL PARADISE\",\n",
    "        \"NIVEA\": \"NIVEA\",\n",
    "        \"NOPIKEX\": \"NOPIKEX\",\n",
    "        \"NOVAVENTA FPT\": \"NOVAVENTA FPT\",\n",
    "        \"NUDE\": \"NUDE\",\n",
    "        \"OGX\": \"OGX\",\n",
    "        \"OLAY\": \"OLAY\",\n",
    "        \"OMNILIFE\": \"OMNILIFE\",\n",
    "        \"OTRAS\": \"OTRAS\",\n",
    "        \"PREBEL\": \"PREBEL\",\n",
    "        \"QVS\": \"ACCESORIOS\",\n",
    "        \"SALLY HANSEN\": \"SALLY HANSEN\",\n",
    "        \"SIN ASIGNAR\": \"SIN ASIGNAR\",\n",
    "        \"SOLLA\": \"SOLLA\",\n",
    "        \"ST. IVES\": \"ST. IVES\",\n",
    "        \"UBU\": \"ACCESORIOS\",\n",
    "        \"UNILEVER\": \"UNILEVER\",\n",
    "        \"VENTA DIRECTA COSM√âTICOS\": \"VENTA DIRECTA COSM√âTICOS\",\n",
    "        \"VIT√ö\": \"VIT√ö\",\n",
    "        \"VIT√ö  EXPORTACI√ìN\": \"VIT√ö\",\n",
    "        \"WELLA CONSUMO\": \"WELLA CONSUMO\",\n",
    "        \"WELLA PROFESSIONAL\": \"WELLA PROFESSIONAL\",\n",
    "        \"YARDLEY\": \"YARDLEY\",\n",
    "        \"CAT√ÅLOGO DE PRODUCTOS\": \"CAT√ÅLOGO DE PRODUCTOS\",\n",
    "        \"D1\": \"D1\",\n",
    "        \"WORMSER\": \"WORMSER\",\n",
    "        \"PROCTER AND GAMBLE\": \"P&G\",\n",
    "        \"DAVINES\": \"DAVINES\",\n",
    "        \"LA FABRIL\": \"LA FABRIL\",\n",
    "        \"REVOX\": \"REVOX\",\n",
    "        \"TENDENCIAS AB\": \"TENDENCIAS AB\",\n",
    "    }\n",
    "\n",
    "\n",
    "def insert_subsegmentacion() -> Dict[str, str]:\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    RETORNA UN DICCIONARIO CON EL MAPEO DE MARCAS QM A SUBSEGMENTACI√ìN.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"ACCESORIOS\": \"OTROS\",\n",
    "        \"ADIDAS\": \"OTROS\",\n",
    "        \"AGATHA RUIZ DE LA PRADA\": \"OTROS\",\n",
    "        \"ALICORP\": \"FULL\",\n",
    "        \"AMAZON\": \"RETAILERS\",\n",
    "        \"AMWAY\": \"SISTEMA DE VENTAS\",\n",
    "        \"ARDEN FOR MEN\": \"OTROS\",\n",
    "        \"AVON\": \"FULL\",\n",
    "        \"BALANCE\": \"FULL\",\n",
    "        \"BANCO PREBEL\": \"FULL\",\n",
    "        \"BEAUTYHOLICS\": \"OTROS\",\n",
    "        \"BIO OIL\": \"OTROS\",\n",
    "        \"BIOTECNIK\": \"FULL\",\n",
    "        \"BURTS_BEES\": \"OTROS\",\n",
    "        \"CADIVEU\": \"PROFESIONALES\",\n",
    "        \"CALA\": \"FULL\",\n",
    "        \"CATRICE\": \"OTROS\",\n",
    "        \"CONNECT FOR MEN\": \"OTROS\",\n",
    "        \"COSMETRIX\": \"OTROS\",\n",
    "        \"COVER GIRL\": \"OTROS\",\n",
    "        \"DIAL\": \"RETAILERS\",\n",
    "        \"DOVE\": \"OTROS\",\n",
    "        \"DYCLASS\": \"SISTEMA DE VENTAS\",\n",
    "        \"ECAR\": \"FULL\",\n",
    "        \"EL EXITO\": \"RETAILERS\",\n",
    "        \"ELIZABETH ARDEN\": \"OTROS\",\n",
    "        \"ESSENCE\": \"OTROS\",\n",
    "        \"FAMILIA\": \"FULL\",\n",
    "        \"FEBREZE\": \"OTROS\",\n",
    "        \"FISA\": \"FULL\",\n",
    "        \"HASK\": \"OTROS\",\n",
    "        \"HENKEL\": \"FULL\",\n",
    "        \"HERBAL ESSENCES\": \"OTROS\",\n",
    "        \"IMPORTADOS PROCTER\": \"OTROS\",\n",
    "        \"JERONIMO MARTINS\": \"RETAILERS\",\n",
    "        \"KANABECARE\": \"OTROS\",\n",
    "        \"KIMBERLY\": \"FULL\",\n",
    "        \"KOBA\": \"RETAILERS\",\n",
    "        \"L&G ASOCIADOS\": \"FULL\",\n",
    "        \"LA POPULAR\": \"RETAILERS\",\n",
    "        \"LEONISA\": \"SISTEMA DE VENTAS\",\n",
    "        \"LOCATEL\": \"RETAILERS\",\n",
    "        \"LOREAL\": \"FULL\",\n",
    "        \"LOVE, BEAUTY AND PLANET\": \"OTROS\",\n",
    "        \"MAUI\": \"OTROS\",\n",
    "        \"MAX FACTOR\": \"OTROS\",\n",
    "        \"MAX FACTOR EXPORTACI√ìN\": \"OTROS\",\n",
    "        \"MAX FACTOR GLOBAL\": \"OTROS\",\n",
    "        \"MILAGROS\": \"SISTEMA DE VENTAS\",\n",
    "        \"MONCLER\": \"RETAILERS\",\n",
    "        \"MORROCCANOIL\": \"PROFESIONALES\",\n",
    "        \"NATURA\": \"FULL\",\n",
    "        \"NATURAL PARADISE\": \"OTROS\",\n",
    "        \"NIVEA\": \"FULL\",\n",
    "        \"NOPIKEX\": \"OTROS\",\n",
    "        \"NOVAVENTA FPT\": \"SISTEMA DE VENTAS\",\n",
    "        \"NUDE\": \"NUDE\",\n",
    "        \"OGX\": \"OTROS\",\n",
    "        \"OLAY\": \"OTROS\",\n",
    "        \"OMNILIFE\": \"SISTEMA DE VENTAS\",\n",
    "        \"OTRAS\": \"OTROS\",\n",
    "        \"PREBEL\": \"OTROS\",\n",
    "        \"QVS\": \"OTROS\",\n",
    "        \"SALLY HANSEN\": \"OTROS\",\n",
    "        \"SIN ASIGNAR\": \"FULL\",\n",
    "        \"SOLLA\": \"FULL\",\n",
    "        \"ST. IVES\": \"OTROS\",\n",
    "        \"UBU\": \"OTROS\",\n",
    "        \"UNILEVER\": \"TOLL\",\n",
    "        \"VENTA DIRECTA COSM√âTICOS\": \"FULL\",\n",
    "        \"VIT√ö\": \"OTROS\",\n",
    "        \"VIT√ö  EXPORTACI√ìN\": \"OTROS\",\n",
    "        \"WELLA CONSUMO\": \"OTROS\",\n",
    "        \"WELLA PROFESSIONAL\": \"PROFESIONALES\",\n",
    "        \"YARDLEY\": \"OTROS\",\n",
    "        \"D1\": \"RETAILERS\",\n",
    "        \"CAT√ÅLOGO DE PRODUCTOS\": \"RETAILERS\",\n",
    "        \"WORMSER\": \"RETAILERS\",\n",
    "        \"PROCTER AND GAMBLE\": \"FULL\",\n",
    "        \"DAVINES\": \"OTROS\",\n",
    "        \"LA FABRIL\": \"OTROS\",\n",
    "        \"REVOX\": \"OTROS\",\n",
    "        \"TENDENCIAS AB\": \"RETAILERS\"\n",
    "    }\n",
    "\n",
    "\n",
    "def insert_segments() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Retorna un diccionario con el mapeo de marcas QM a segmentaciones.\n",
    "    \"\"\"\n",
    "    # TODO: Implementar el diccionario de segmentaciones seg√∫n la l√≥gica de negocio\n",
    "    return {\n",
    "        \"OTROS CLIENTES DO\": \"DUE√ëOS DE CANAL\",\n",
    "        \"MARKETING PERSONAL\": \"DUE√ëOS DE CANAL\",\n",
    "        \"OMNILIFE\": \"DUE√ëOS DE CANAL\",\n",
    "        \"JERONIMO MARTINS\": \"DUE√ëOS DE CANAL\",\n",
    "        \"LEONISA\": \"DUE√ëOS DE CANAL\",\n",
    "        \"LOCATEL\": \"DUE√ëOS DE CANAL\",\n",
    "        \"NOVAVENTA\": \"DUE√ëOS DE CANAL\",\n",
    "        \"EL √âXITO\": \"DUE√ëOS DE CANAL\",\n",
    "        \"MILAGROS ENTERPRISE\": \"DUE√ëOS DE CANAL\",\n",
    "        \"LA POPULAR\": \"DUE√ëOS DE CANAL\",\n",
    "        \"D1\": \"DUE√ëOS DE CANAL\",\n",
    "        \"USA\": \"DUE√ëOS DE CANAL\",\n",
    "        \"USA\": \"DUE√ëOS DE CANAL\",\n",
    "        \"EL EXITO\": \"DUE√ëOS DE CANAL\",\n",
    "        \"NOVAVENTA FPT\": \"DUE√ëOS DE CANAL\",\n",
    "        \"MILAGROS\": \"DUE√ëOS DE CANAL\",\n",
    "        \"WORMSER\": \"DUE√ëOS DE CANAL\",\n",
    "        \"TENDENCIAS AB\": \"DUE√ëOS DE CANAL\",\n",
    "        \"LA FABRIL\": \"DUE√ëOS DE CANAL\",\n",
    "        \"UNILEVER\": \"EXPERTOS LOCALES\",\n",
    "        \"NATURA\": \"EXPERTOS LOCALES\",\n",
    "        \"BIOTECNIK\": \"EXPERTOS LOCALES\",\n",
    "        \"NIVEA\": \"EXPERTOS LOCALES\",\n",
    "        \"BRITO\": \"EXPERTOS LOCALES\",\n",
    "        \"AVON\": \"EXPERTOS LOCALES\",\n",
    "        \"OTROS EXPERTOS LOCALES\": \"EXPERTOS LOCALES\",\n",
    "        \"ALICORP\": \"EXPERTOS LOCALES\",\n",
    "        \"SOLLA\": \"EXPERTOS LOCALES\",\n",
    "        \"ECAR\": \"EXPERTOS LOCALES\",\n",
    "        \"FISA\": \"EXPERTOS LOCALES\",\n",
    "        \"KIMBERLY\": \"EXPERTOS LOCALES\",\n",
    "        \"BELCORP\": \"EXPERTOS LOCALES\",\n",
    "        \"AMWAY\": \"EXPERTOS LOCALES\",\n",
    "        \"PROCTER AND GAMBLE\": \"EXPERTOS LOCALES\",\n",
    "        \"HENKEL\": \"EXPERTOS LOCALES\",\n",
    "        \"DIAL\": \"EXPERTOS LOCALES\",\n",
    "        \"BEIERSDORF\": \"EXPERTOS LOCALES\",\n",
    "        \"OTROS EXPERTOS LOCALES\": \"EXPERTOS LOCALES\",\n",
    "        \"FAMILIA\": \"EXPERTOS LOCALES\",\n",
    "        \"BALANCE\": \"EXPERTOS LOCALES\",\n",
    "        \"MAX FACTOR\": \"EXPERTOS NO LOCALES\",\n",
    "        \"DYCLASS\": \"EXPERTOS NO LOCALES\",\n",
    "        \"WELLA CONSUMO\": \"EXPERTOS NO LOCALES\",\n",
    "        \"BIO OIL\": \"EXPERTOS NO LOCALES\",\n",
    "        \"OGX\": \"EXPERTOS NO LOCALES\",\n",
    "        \"COVER GIRL\": \"EXPERTOS NO LOCALES\",\n",
    "        \"WELLA PROFESSIONAL\": \"EXPERTOS NO LOCALES\",\n",
    "        \"ADIDAS\": \"EXPERTOS NO LOCALES\",\n",
    "        \"ACCESORIOS\": \"EXPERTOS NO LOCALES\",\n",
    "        \"BURTS_BEES\": \"EXPERTOS NO LOCALES\",\n",
    "        \"NOPIKEX\": \"EXPERTOS NO LOCALES\",\n",
    "        \"QVS\": \"EXPERTOS NO LOCALES\",\n",
    "        \"UBU\": \"EXPERTOS NO LOCALES\",\n",
    "        \"ESSENCE\": \"EXPERTOS NO LOCALES\",\n",
    "        \"MORROCCANOIL\": \"EXPERTOS NO LOCALES\",\n",
    "        \"HASK\": \"EXPERTOS NO LOCALES\",\n",
    "        \"HERBAL ESSENCES\": \"EXPERTOS NO LOCALES\",\n",
    "        \"LOVE, BEAUTY AND PLANET\": \"EXPERTOS NO LOCALES\",\n",
    "        \"CATRICE\": \"EXPERTOS NO LOCALES\",\n",
    "        \"NATURAL PARADISE\": \"EXPERTOS NO LOCALES\",\n",
    "        \"OLAY\": \"EXPERTOS NO LOCALES\",\n",
    "        \"MID\": \"EXPERTOS NO LOCALES\",\n",
    "        \"SECRET\": \"EXPERTOS NO LOCALES\",\n",
    "        \"FEBREZE\": \"EXPERTOS NO LOCALES\",\n",
    "        \"TAMPAX\": \"EXPERTOS NO LOCALES\",\n",
    "        \"OFCORSS C.I HERMECO\": \"EXPERTOS NO LOCALES\",\n",
    "        \"CADIVEU\": \"EXPERTOS NO LOCALES\",\n",
    "        \"MAX FACTOR GLOBAL\": \"EXPERTOS NO LOCALES\",\n",
    "        \"SEBASTIAN\": \"EXPERTOS NO LOCALES\",\n",
    "        \"AFFRESH\": \"EXPERTOS NO LOCALES\",\n",
    "        \"COSMETRIX\": \"EXPERTOS NO LOCALES\",\n",
    "        \"INCENTIVOS MAX FACTOR\": \"EXPERTOS NO LOCALES\",\n",
    "        \"OTROS EXPERTOS NO LOCALES\": \"EXPERTOS NO LOCALES\",\n",
    "        \"DAVINES\": \"EXPERTOS NO LOCALES\",\n",
    "        \"P&G\": \"EXPERTOS NO LOCALES\",\n",
    "        \"REVOX\": \"EXPERTOS NO LOCALES\",\n",
    "        \"UTOPICK\": \"EXPERTOS NO LOCALES\",\n",
    "        \"IMPORTADOS PROCTER\": \"EXPERTOS NO LOCALES\",\n",
    "        \"ST. IVES\": \"EXPERTOS NO LOCALES\",\n",
    "        \"ARDEN FOR MEN\": \"MARCAS PROPIAS\",\n",
    "        \"NUDE\": \"MARCAS PROPIAS\",\n",
    "        \"ELIZABETH ARDEN\": \"MARCAS PROPIAS\",\n",
    "        \"YARDLEY\": \"MARCAS PROPIAS\",\n",
    "        \"VIT√ö\": \"MARCAS PROPIAS\",\n",
    "        \"PREBEL\": \"MARCAS PROPIAS\",\n",
    "        \"OTRAS MP\": \"MARCAS PROPIAS\",\n",
    "        \"AFM/CFM\": \"MARCAS PROPIAS\",\n",
    "        \"BODY CLEAR\": \"NO APLICA\",\n",
    "        \"OTRAS\": \"NO APLICA\",\n",
    "        \"GILLETTE\": \"NO APLICA\",\n",
    "        \"L&G ASOCIADOS\": \"NO APLICA\",\n",
    "        \"CAT√ÅLOGO DE PRODUCTOS\": \"NO APLICA\",\n",
    "        \"HINODE\": \"NO APLICA\",\n",
    "        \"PFIZER\": \"NO APLICA\",\n",
    "        \"CONTEXPORT DISNEY\": \"NO APLICA\",\n",
    "        \"SYSTEM PROFESSIONAL\": \"NO APLICA\",\n",
    "        \"WELONDA\": \"NO APLICA\",\n",
    "        \"SIN ASIGNAR\": \"NO APLICA\",\n",
    "        \"CAT√ÅLOGO DE PRODUCTOS\": \"NO APLICA\",\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_rango_permanencia(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Calcula el rango de permanencia basado en las condiciones especificadas.\n",
    "\n",
    "    Args:\n",
    "        row: Fila del DataFrame con las columnas necesarias\n",
    "\n",
    "    Returns:\n",
    "        str: Rango de permanencia calculado\n",
    "    \"\"\"\n",
    "    lote = row.get(\"LOTE\", None)\n",
    "    permanencia = row.get(\"PERMANENCIA\", None)\n",
    "    rango_permanencia = row.get(\"RANGO DE PERMANENCIA\", None)\n",
    "\n",
    "    if lote == \"222222\":\n",
    "        return \"1.MENOR DE 90 DIAS\"\n",
    "    elif permanencia == 0 and rango_permanencia == \"5.MAYOR O IGUAL A 360 DIAS\":\n",
    "        return \"5.ENTRE 360 Y 540 DIAS\"\n",
    "    elif rango_permanencia == \"5.MAYOR O IGUAL A 360 DIAS\":\n",
    "        if permanencia < 540:\n",
    "            return \"5.ENTRE 360 Y 540 DIAS\"\n",
    "        elif permanencia < 720:\n",
    "            return \"6.ENTRE 540 Y 720 DIAS\"\n",
    "        else:\n",
    "            return \"7.MAYOR DE 720 DIAS\"\n",
    "    else:\n",
    "        return rango_permanencia\n",
    "\n",
    "\n",
    "def calculate_status_cons(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Calcula el estado de consumo basado en las condiciones de vencimiento, bloqueo y obsolescencia.\n",
    "\n",
    "    Args:\n",
    "        row: Fila del DataFrame con las columnas necesarias\n",
    "\n",
    "    Returns:\n",
    "        str: Estado calculado (VENCIDO, BLOQUEADO, OBSOLETO, PAV o DISPONIBLE)\n",
    "    \"\"\"\n",
    "    rango_prox_vencer = row.get(\"RANGO PR√ìX.VENCER MM\")\n",
    "    valor_BLOQUEADO = row.get(\"VALOR BLOQUEADO MM\")\n",
    "    valor_OBSOLETO = row.get(\"VALOR OBSOLETO\")\n",
    "\n",
    "    if rango_prox_vencer == \"VENCIDO\":\n",
    "        return \"VENCIDO\"\n",
    "    elif valor_BLOQUEADO != 0:\n",
    "        return \"BLOQUEADO\"\n",
    "    elif valor_OBSOLETO != 0:\n",
    "        return \"OBSOLETO\"\n",
    "    elif rango_prox_vencer in [\"1.PAV 3 MESES\", \"2.PAV 4 A 6 MESES\"]:\n",
    "        return \"PAV\"\n",
    "    else:\n",
    "        return \"DISPONIBLE\"\n",
    "\n",
    "\n",
    "def calculate_valor_def(row: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Calcula el valor definitivo basado en el estado de consumo.\n",
    "\n",
    "    Args:\n",
    "        row: Fila del DataFrame con las columnas necesarias\n",
    "\n",
    "    Returns:\n",
    "        float: Valor definitivo calculado\n",
    "    \"\"\"\n",
    "    status_cons = row.get(\"STATUS CONS\")\n",
    "    valor_BLOQUEADO = row.get(\"VALOR BLOQUEADO MM\")\n",
    "    valor_total = row.get(\"VALOR TOTAL MM\")\n",
    "\n",
    "    return valor_BLOQUEADO if status_cons == \"BLOQUEADO\" else valor_total\n",
    "\n",
    "\n",
    "def calculate_rango_obsolescencia(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Calcula el rango de obsolescencia basado en las fechas de entrada y obsolescencia.\n",
    "\n",
    "    Args:\n",
    "        row: Fila del DataFrame con las columnas necesarias\n",
    "\n",
    "    Returns:\n",
    "        str: Rango de obsolescencia calculado\n",
    "    \"\"\"\n",
    "    status_cons = row.get(\"STATUS CONS\")\n",
    "    fecha_entrada = row.get(\"FECHA ENTRADA\")\n",
    "    fecha_OBSOLETO = row.get(\"FECHA OBSOLETO\")\n",
    "\n",
    "    if status_cons != \"OBSOLETO\" or pd.isna(fecha_entrada) or pd.isna(fecha_OBSOLETO):\n",
    "        return \"FALSO\"\n",
    "\n",
    "    dias_obsolescencia = (fecha_entrada - fecha_OBSOLETO).days\n",
    "\n",
    "    if dias_obsolescencia <= 90:\n",
    "        return \"1.MENOR DE 90 DIAS\"\n",
    "    elif 90 < dias_obsolescencia <= 180:\n",
    "        return \"2.ENTRE 90 Y 180 DIAS\"\n",
    "    elif 180 < dias_obsolescencia <= 270:\n",
    "        return \"3.ENTRE 180 Y 270 DIAS\"\n",
    "    elif 270 < dias_obsolescencia <= 360:\n",
    "        return \"4.ENTRE 270 Y 360 DIAS\"\n",
    "    elif 360 < dias_obsolescencia <= 540:\n",
    "        return \"5.ENTRE 360 Y 540 DIAS\"\n",
    "    elif 540 < dias_obsolescencia <= 720:\n",
    "        return \"6.ENTRE 540 Y 720 DIAS\"\n",
    "    else:\n",
    "        return \"7.MAYOR DE 720 DIAS\"\n",
    "\n",
    "\n",
    "def calculate_rango_vencido(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Calcula el rango de vencimiento basado en las fechas de entrada y caducidad.\n",
    "\n",
    "    Args:\n",
    "        row: Fila del DataFrame con las columnas necesarias\n",
    "\n",
    "    Returns:\n",
    "        str: Rango de vencimiento calculado\n",
    "    \"\"\"\n",
    "    status_cons = row.get(\"STATUS CONS\")\n",
    "    fecha_entrada = row.get(\"FECHA ENTRADA\")\n",
    "    fecha_caducidad = row.get(\"FECH, CADUCIDAD/FECH PREF. CONSUMO\")\n",
    "\n",
    "    if status_cons != \"VENCIDO\" or pd.isna(fecha_entrada) or pd.isna(fecha_caducidad):\n",
    "        return \"FALSO\"\n",
    "\n",
    "    dias_vencido = (fecha_entrada - fecha_caducidad).days\n",
    "\n",
    "    if dias_vencido <= 90:\n",
    "        return \"1.MENOR DE 90 DIAS\"\n",
    "    elif 90 < dias_vencido <= 180:\n",
    "        return \"2.ENTRE 90 Y 180 DIAS\"\n",
    "    elif 180 < dias_vencido <= 270:\n",
    "        return \"3.ENTRE 180 Y 270 DIAS\"\n",
    "    elif 270 < dias_vencido <= 360:\n",
    "        return \"4.ENTRE 270 Y 360 DIAS\"\n",
    "    elif 360 < dias_vencido <= 540:\n",
    "        return \"5.ENTRE 360 Y 540 DIAS\"\n",
    "    elif 540 < dias_vencido <= 720:\n",
    "        return \"6.ENTRE 540 Y 720 DIAS\"\n",
    "    else:\n",
    "        return \"7.MAYOR DE 720 DIAS\"\n",
    "\n",
    "\n",
    "def calculate_rango_bloqueado(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Calcula el rango de bloqueo basado en las fechas de entrada y bloqueo.\n",
    "\n",
    "    Args:\n",
    "        row: Fila del DataFrame con las columnas necesarias\n",
    "\n",
    "    Returns:\n",
    "        str: Rango de bloqueo calculado\n",
    "    \"\"\"\n",
    "    status_cons = row.get(\"STATUS CONS\")\n",
    "    fecha_entrada = row.get(\"FECHA ENTRADA\")\n",
    "    fecha_bloqueado = row.get(\"FECHA BLOQUEADO\")\n",
    "\n",
    "    if status_cons != \"BLOQUEADO\" or pd.isna(fecha_entrada) or pd.isna(fecha_bloqueado):\n",
    "        return \"FALSO\"\n",
    "\n",
    "    dias_vencido = (fecha_entrada - fecha_bloqueado).days\n",
    "\n",
    "    if dias_vencido <= 90:\n",
    "        return \"1.MENOR DE 90 DIAS\"\n",
    "    elif 90 < dias_vencido <= 180:\n",
    "        return \"2.ENTRE 90 Y 180 DIAS\"\n",
    "    elif 180 < dias_vencido <= 270:\n",
    "        return \"3.ENTRE 180 Y 270 DIAS\"\n",
    "    elif 270 < dias_vencido <= 360:\n",
    "        return \"4.ENTRE 270 Y 360 DIAS\"\n",
    "    elif 360 < dias_vencido <= 540:\n",
    "        return \"5.ENTRE 360 Y 540 DIAS\"\n",
    "    elif 540 < dias_vencido <= 720:\n",
    "        return \"6.ENTRE 540 Y 720 DIAS\"\n",
    "    else:\n",
    "        return \"7.MAYOR A 720 DIAS\"\n",
    "\n",
    "\n",
    "def calculate_tiempo_bloqueo(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Calcula el tiempo de bloqueo basado en las fechas de entrada y bloqueo.\n",
    "\n",
    "    Args:\n",
    "        row: Fila del DataFrame con las columnas necesarias\n",
    "\n",
    "    Returns:\n",
    "        str: Tiempo de bloqueo calculado\n",
    "    \"\"\"\n",
    "    fecha_entrada = row.get(\"FECHA ENTRADA\")\n",
    "    fecha_bloqueado = row.get(\"FECHA BLOQUEADO\")\n",
    "    \n",
    "    if pd.isna(fecha_entrada) or pd.isna(fecha_bloqueado):\n",
    "        return 0\n",
    "\n",
    "    tiempo_bloqueado = (fecha_entrada - fecha_bloqueado).days\n",
    "    \n",
    "    return tiempo_bloqueado    \n",
    "\n",
    "\n",
    "def calculate_rango_cons(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Calcula el rango de consumo final basado en el status y los rangos correspondientes.\n",
    "\n",
    "    Args:\n",
    "        row: Fila del DataFrame con las columnas necesarias\n",
    "\n",
    "    Returns:\n",
    "        str: Rango de consumo calculado\n",
    "    \"\"\"\n",
    "    status = row.get(\"STATUS CONS\")\n",
    "\n",
    "    if status == \"OBSOLETO\":\n",
    "        return row.get(\"RANGO OBSOLESCENCIA\")\n",
    "    elif status == \"VENCIDO\":\n",
    "        return row.get(\"RANGO VENCIDO 2\")\n",
    "    elif status == \"BLOQUEADO\":\n",
    "        return row.get(\"RANGO BLOQUEADO 2\")\n",
    "    elif status == \"PAV\":\n",
    "        return row.get(\"RANGO DE PERMANENCIA 2\")\n",
    "    else:\n",
    "        return row.get(\"RANGO DE PERMANENCIA 2\")\n",
    "\n",
    "\n",
    "def calculate_otros_marcas_factor_and_class(row: pd.Series, lookup_dict: dict) -> tuple[float, str]:\n",
    "    \"\"\"\n",
    "    Calcula factor provisional y clasificaci√≥n para el resto de marcas\n",
    "    (excluyendo Avon/Natura), siguiendo la f√≥rmula de Excel proporcionada.\n",
    "    \"\"\"\n",
    "    seg         = row[\"SEGMENTACION\"]\n",
    "    status      = row[\"STATUS CONS\"]\n",
    "    tiempo      = row[\"TIEMPO BLOQUEO\"]\n",
    "    indic       = row[\"INDICADOR STOCK ESPEC.\"]\n",
    "    perm        = row[\"PERMANENCIA\"]\n",
    "    tipo_mat    = row[\"TIPO DE MATERIAL (I)\"]\n",
    "    rango_cons  = str(row[\"RANGO CONS\"]).strip()\n",
    "    cobertura   = str(row[\"RANGO COBERTURA\"]).strip()\n",
    "    subseg      = str(row.get(\"SUBSEGMENTACION\", \"\")).strip()\n",
    "    prox_vencer = str(row.get(\"RANGO PR√ìX.VENCER MM\", \"\")).strip()\n",
    "    negocio     = row[\"NEGOCIO INVENTARIOS\"]\n",
    "\n",
    "    # 1) Due√±os de canal / Marcas propias / Expertos no locales + Bloqueado corto ‚Üí 0%, BAJO\n",
    "    if seg in [\"DUE√ëOS DE CANAL\", \"MARCAS PROPIAS\", \"EXPERTOS NO LOCALES\"] \\\n",
    "       and status == \"BLOQUEADO\" \\\n",
    "       and tiempo <= 30:\n",
    "        return 0.0, \"BAJO\"\n",
    "\n",
    "    # 2) Indicador stock = \"K\" ‚Üí 0%, BAJO\n",
    "    if indic == \"K\":\n",
    "        return 0.0, \"BAJO\"\n",
    "\n",
    "    # 3) Disponible/PAV + Granel y permanencia corta ‚Üí 0%, BAJO\n",
    "    if status in [\"DISPONIBLE\", \"PAV\"] \\\n",
    "       and tipo_mat in [\"GRANEL\", \"GRANEL FAB A TERCERO\"] \\\n",
    "       and perm <= 30:\n",
    "        return 0.0, \"BAJO\"\n",
    "\n",
    "    # 4) Marca \"OTRAS\" o (Disponible + cobertura \"\") ‚Üí 0%, BAJO\n",
    "    if row[\"MARCA DE QM\"] == \"OTRAS\" \\\n",
    "       or (status == \"DISPONIBLE\" and cobertura == \"\"):\n",
    "        return 0.0, \"BAJO\"\n",
    "\n",
    "    # 5) Stock = \"W\" ‚Üí lookup seg√∫n status\n",
    "    if indic == \"W\":\n",
    "        if status in [\"VENCIDO\", \"PAV\"]:\n",
    "            key = f\"{seg}{status}{row['RANGO DE PERMANENCIA 2']}\"\n",
    "        elif status == \"DISPONIBLE\":\n",
    "            key = f\"{seg}{cobertura}{row['RANGO DE PERMANENCIA 2']}\"\n",
    "        else:\n",
    "            key = f\"{seg}{status}{rango_cons}\"\n",
    "        return lookup_dict.get(key, (0.0, \"BAJO\"))\n",
    "\n",
    "    # 6) Stock = \"SIN ASIGNAR\" u \"O\"\n",
    "    if indic in [\"SIN ASIGNAR\", \"O\"]:\n",
    "        # a) PAV + pr√≥xima a vencer 3 o 4-6 meses\n",
    "        if status == \"PAV\" and prox_vencer in [\"1.PAV 3 MESES\", \"2.PAV 4 A 6 MESES\"]:\n",
    "            key = f\"{seg}{subseg}{cobertura}{row['RANGO DE PERMANENCIA 2']}\"\n",
    "            return lookup_dict.get(key, (0.0, \"BAJO\"))\n",
    "        # b) Disponible ‚Üí lookup principal y fallback\n",
    "        if status == \"DISPONIBLE\":\n",
    "            key = f\"{seg}{subseg}{cobertura}{row['RANGO DE PERMANENCIA 2']}\"\n",
    "            if key in lookup_dict:\n",
    "                return lookup_dict[key]\n",
    "            alt = f\"{seg}{subseg}{status} {rango_cons}\"\n",
    "            return lookup_dict.get(alt, (0.0, \"BAJO\"))\n",
    "        # c) Obsoleto/Vencido/Bloqueado\n",
    "        if status in [\"OBSOLETO\", \"VENCIDO\", \"BLOQUEADO\"]:\n",
    "            key = f\"{seg}{subseg}{status} {rango_cons}\"\n",
    "            return lookup_dict.get(key, (0.0, \"BAJO\"))\n",
    "\n",
    "    # 7) Fallback por defecto ‚Üí 0%, BAJO\n",
    "    return 0.0, \"BAJO\"\n",
    "\n",
    "\n",
    "def calculate_base_riesgo_column(row: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Calcula el valor de la columna de base riesgo\n",
    "    \"\"\"\n",
    "    if row[\"CLAS BASE RIESGO\"] == \"BAJO\":\n",
    "        return 0.0\n",
    "    else:\n",
    "        return row[\"VALOR DEF\"]\n",
    "\n",
    "\n",
    "def calculate_provision_column(row: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Calcula el valor de la columna de provisi√≥n\n",
    "    \"\"\"\n",
    "    if row[\"MARCA DE QM\"] == \"OTRAS\":\n",
    "        return 0.0\n",
    "    else:\n",
    "        return row[\"VALOR DEF\"] * row['FACTOR PROV']\n",
    "\n",
    "\n",
    "def process_dataframe_otras_marcas(df_otras_marcas: pd.DataFrame,df_matrices_otros_tipos: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Procesa el DataFrame aplicando todas las reglas de negocio en el orden espec√≠fico requerido.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame con los datos de SAP\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame procesado con todas las columnas calculadas\n",
    "    \"\"\"\n",
    "    # 1. A√±adir columnas formuladas de 'MARCA CONCAT' y 'SEGMENTACION'\n",
    "    df_otras_marcas[\"MARCA CONCAT\"] = df_otras_marcas[\"MARCA DE QM\"].apply(lambda x: insert_marks().get(x, \"\"))\n",
    "    df_otras_marcas[\"SEGMENTACION\"] = df_otras_marcas[\"MARCA DE QM\"].apply(\n",
    "        lambda x: insert_segments().get(x, \"OTRAS\")\n",
    "    )\n",
    "    df_otras_marcas[\"SUBSEGMENTACION\"] = df_otras_marcas[\"MARCA DE QM\"].apply(\n",
    "        lambda x: insert_subsegmentacion().get(x, \"\")\n",
    "    )\n",
    "\n",
    "    # 2. Calcular 'RANGO DE PERMANENCIA 2'\n",
    "    required_columns = {\"LOTE\", \"PERMANENCIA\", \"RANGO DE PERMANENCIA\"}\n",
    "    if required_columns.issubset(df_otras_marcas.columns):\n",
    "        df_otras_marcas[\"RANGO DE PERMANENCIA 2\"] = df_otras_marcas.apply(calculate_rango_permanencia, axis=1)\n",
    "    else:\n",
    "        print(f\"Faltan columnas: {required_columns - set(df_otras_marcas.columns)}\")\n",
    "\n",
    "    # 3. Calcular 'STATUS CONS'\n",
    "    required_columns = {\"RANGO PR√ìX.VENCER MM\", \"VALOR BLOQUEADO MM\", \"VALOR OBSOLETO\"}\n",
    "    if required_columns.issubset(df_otras_marcas.columns):\n",
    "        df_otras_marcas[\"STATUS CONS\"] = df_otras_marcas.apply(calculate_status_cons, axis=1)\n",
    "    else:\n",
    "        print(f\"Faltan columnas: {required_columns - set(df_otras_marcas.columns)}\")\n",
    "\n",
    "    # 4. Calcular 'VALOR DEF'\n",
    "    required_columns = {\"STATUS CONS\", \"VALOR BLOQUEADO MM\", \"VALOR TOTAL MM\"}\n",
    "    if required_columns.issubset(df_otras_marcas.columns):\n",
    "        df_otras_marcas[\"VALOR DEF\"] = df_otras_marcas.apply(calculate_valor_def, axis=1)\n",
    "    else:\n",
    "        print(f\"Faltan columnas: {required_columns - set(df_otras_marcas.columns)}\")\n",
    "\n",
    "    # 5. Reemplazar valores inv√°lidos\n",
    "    df_otras_marcas.replace(\"#\", np.nan, inplace=True)\n",
    "\n",
    "    # 6. Convertir columnas de fecha\n",
    "    date_columns = [\n",
    "        \"FECHA ENTRADA\",\n",
    "        \"FECHA OBSOLETO\",\n",
    "        \"FECHA BLOQUEADO\",\n",
    "        \"FECH. FABRICACI√ìN\",\n",
    "        \"CREADO EL\",\n",
    "        \"FECH, CADUCIDAD/FECH PREF. CONSUMO\",\n",
    "    ]\n",
    "\n",
    "    for col in date_columns:\n",
    "        if col in df_otras_marcas.columns:\n",
    "            df_otras_marcas[col] = pd.to_datetime(df_otras_marcas[col], format=\"%d/%m/%Y\", errors=\"coerce\")\n",
    "\n",
    "    # 7. Calcular 'RANGO OBSOLESCENCIA'\n",
    "    df_otras_marcas[\"RANGO OBSOLESCENCIA\"] = df_otras_marcas.apply(calculate_rango_obsolescencia, axis=1)\n",
    "\n",
    "    # 8. Calcular 'RANGO VENCIDO 2'\n",
    "    df_otras_marcas[\"RANGO VENCIDO 2\"] = df_otras_marcas.apply(calculate_rango_vencido, axis=1)\n",
    "\n",
    "    # 9. Calcular 'RANGO BLOQUEADO 2'\n",
    "    df_otras_marcas[\"RANGO BLOQUEADO 2\"] = df_otras_marcas.apply(calculate_rango_bloqueado, axis=1)\n",
    "\n",
    "    # 10. Calcular 'RANGO CONS'\n",
    "    df_otras_marcas[\"RANGO CONS\"] = df_otras_marcas.apply(calculate_rango_cons, axis=1)\n",
    "    \n",
    "    # 11. Calcular 'TIEMPO BLOQUEO'\n",
    "    df_otras_marcas[\"TIEMPO BLOQUEO\"] = df_otras_marcas.apply(calculate_tiempo_bloqueo, axis=1)\n",
    "\n",
    "    # Construir lookup_dict **una vez** antes del apply\n",
    "    lookup_dict = {\n",
    "        str(r[\"concatenado\"]).strip(): (r[\"factor_prov\"], r[\"clasificacion\"])\n",
    "        for _, r in df_matrices_otros_tipos.iterrows()\n",
    "    }\n",
    "\n",
    "    # Aplicar fila a fila y asignar dos nuevas columnas\n",
    "    df_otras_marcas[[\"FACTOR PROV\", \"CLAS BASE RIESGO\"]] = df_otras_marcas.apply(\n",
    "        lambda row: pd.Series(calculate_otros_marcas_factor_and_class(row, lookup_dict)),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # 13. Calcular 'BASE RIESGO'\n",
    "    df_otras_marcas[\"BASE RIESGO\"] = df_otras_marcas.apply(calculate_base_riesgo_column, axis=1)\n",
    "    \n",
    "    # 14. Calcular 'PROVISION'\n",
    "    df_otras_marcas[\"PROVISION\"] = df_otras_marcas.apply(calculate_provision_column, axis=1)\n",
    "    \n",
    "    return df_otras_marcas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_otras_marcas= process_dataframe_otras_marcas(df_otras_marcas, df_matrices_otros_tipos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_final_dataframes(\n",
    "    df_final_avon_natura: pd.DataFrame,\n",
    "    df_final_otras_marcas: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combina en un √∫nico DataFrame los resultados procesados para:\n",
    "      - Avon/Natura (df_final_avon_natura)\n",
    "      - Resto de marcas (df_final_otras_marcas)\n",
    "\n",
    "    Devuelve un nuevo DataFrame con todos los registros y reinicia el √≠ndice.\n",
    "    \"\"\"\n",
    "    # Verificar que tengan las mismas columnas\n",
    "    cols1 = list(df_final_avon_natura.columns)\n",
    "    cols2 = list(df_final_otras_marcas.columns)\n",
    "    if cols1 != cols2:\n",
    "        raise ValueError(\n",
    "            \"Los DataFrames no coinciden en sus columnas.\\n\"\n",
    "            f\"Avon/Natura: {cols1}\\n\"\n",
    "            f\"Otras marcas: {cols2}\"\n",
    "        )\n",
    "\n",
    "    # Concatenar uno encima del otro\n",
    "    df_final_merge = pd.concat([df_final_avon_natura, df_final_otras_marcas], ignore_index=True)\n",
    "    \n",
    "    # Columnas en el orden deseado\n",
    "    columnas_ordenadas = [\n",
    "        \"NEGOCIO INVENTARIOS\", \"A√ëO NATURAL/MES\",\"TIPO MATERIAL INVENTARIO\", \"MARCA DE QM\", \"MATERIAL\", \n",
    "        \"DESCRIPCI√ìN\", \"UNIDAD MEDIDA\", \"CENTRO\", \"CODIGO ALMACEN CLIENTE\", \"INDICADOR STOCK ESPEC.\",\n",
    "        \"N√öM.STOCK.ESP.\", \"LOTE\", \"CREADO EL\", \"FECH. FABRICACI√ìN\", \"FECH, CADUCIDAD/FECH PREF. CONSUMO\",\n",
    "        \"FECHA BLOQUEADO\", \"FECHA OBSOLETO\", \"FECHA ENTRADA\", \"RANGO OBSOLETO 2\", \"RANGO COBERTURA\",\n",
    "        \"RANGO DE PERMANENCIA\", \"RANGO BLOQUEADO\", \"RANGO OBSOLETO\", \"RANGO VENCIDOS\", \"PR√ìXIMO A VENCER\",\n",
    "        \"RANGO PR√ìX.VENCER MM\", \"RANGO PR√ìXIMOS A VEN\", \"TIPO DE MATERIAL (I)\", \"COSTO UNITARIO REAL\",\n",
    "        \"INVENTARIO DISPONIBL\", \"INVENTARIO NO DISPON\", \"VALOR OBSOLETO\", \"VALOR BLOQUEADO MM\", \"VALOR TOTAL MM\", \"PERMANENCIA\",\n",
    "\n",
    "        \"MARCA CONCAT\", \"SEGMENTACION\", \"SUBSEGMENTACION\",  \n",
    "        \"RANGO DE PERMANENCIA 2\",\n",
    "        \"STATUS CONS\", \"VALOR DEF\", \"RANGO OBSOLESCENCIA\", \"RANGO VENCIDO 2\",\n",
    "        \"RANGO BLOQUEADO 2\", \"RANGO CONS\", \"TIEMPO BLOQUEO\", \n",
    "        \"FACTOR PROV\", \"CLAS BASE RIESGO\", \"BASE RIESGO\", \"PROVISION\" \n",
    "    ]\n",
    "    \n",
    "    # Renombrar columnas duplicadas autom√°ticamente\n",
    "    nuevos_nombres = []\n",
    "    conteo = {}\n",
    "    for col in df_final_merge.columns:\n",
    "        if col in conteo:\n",
    "            conteo[col] += 1\n",
    "            nuevos_nombres.append(f\"{col}_{conteo[col]}\")\n",
    "        else:\n",
    "            conteo[col] = 0\n",
    "            nuevos_nombres.append(col)\n",
    "    df_final_merge.columns = nuevos_nombres\n",
    "    \n",
    "    df_final_merge = df_final_merge.reindex(columns=columnas_ordenadas)\n",
    "\n",
    "    return df_final_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo Excel creado exitosamente: C:\\Users\\prac.planeacionfi\\OneDrive - Prebel S.A BIC\\Escritorio\\PRUEBAS BASE RIESGO\\An√°lisis_BaseRiesgo_Final_08-05-2025.xlsx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\prac.planeacionfi\\\\OneDrive - Prebel S.A BIC\\\\Escritorio\\\\PRUEBAS BASE RIESGO\\\\An√°lisis_BaseRiesgo_Final_08-05-2025.xlsx'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_merge = combine_final_dataframes(df_final_avon_natura, df_final_otras_marcas)\n",
    "export_dataframe_to_excel(df_final_merge)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
